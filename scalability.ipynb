{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff190f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:30.479756Z",
     "start_time": "2025-11-06T06:18:19.588713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egor_demin/Documents/University/TUDelft/GML/gml-project/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, math, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pandas import read_csv\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Intel MKL WARNING\")\n",
    "os.environ['MKL_DEBUG_CPU_TYPE'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f56907e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.25.2\n",
      "torch: 2.2.2 cuda: None\n",
      "NeighborSampler import OK\n",
      "pyg_lib import FAIL\n"
     ]
    }
   ],
   "source": [
    "import numpy, torch, traceback\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "try:\n",
    "    from torch_geometric.loader import NeighborSampler\n",
    "    print(\"NeighborSampler import OK\")\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "try:\n",
    "    import pyg_lib\n",
    "    print(\"pyg_lib import OK\", getattr(pyg_lib,'__version__',None))\n",
    "except Exception:\n",
    "    print(\"pyg_lib import FAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d61ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:30.503157Z",
     "start_time": "2025-11-06T06:18:30.497788Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "EDGE_COLS = [\n",
    "    'Bwd Packet Length Min', 'Protocol_6', 'Bwd Packets/s', 'FWD Init Win Bytes',\n",
    "    'Packet Length Std', 'FIN Flag Count', 'SrcPortRange_registered',\n",
    "    'Packet Length Min', 'Fwd Seg Size Min', 'DstPortRange_well_known',\n",
    "    'Bwd IAT Total', 'SYN Flag Count', 'Bwd Packet Length Std'\n",
    "]\n",
    "ID_COLS = ['Src IP','Dst IP','Timestamp']\n",
    "LABEL_COL = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3662ee25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:22:06.830670Z",
     "start_time": "2025-11-06T06:22:06.819610Z"
    }
   },
   "outputs": [],
   "source": [
    "# def time_posenc(t, periods=(60, 300, 3600)):\n",
    "#     # t: numpy array of epoch seconds\n",
    "#     feats = []\n",
    "#     for P in periods:\n",
    "#         w = 2*math.pi/P\n",
    "#         feats.append(np.sin(w*t))\n",
    "#         feats.append(np.cos(w*t))\n",
    "#     return np.stack(feats, axis=1)  # [N, 2*len(periods)]\n",
    "\n",
    "def bin_time(df, bin_seconds=300):\n",
    "    # Expect df['Timestamp'] as datetime or string; convert to seconds\n",
    "    ts = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True).astype('int64') // 10**9\n",
    "    df = df.copy()\n",
    "    df['_epoch'] = ts\n",
    "    df['_bin'] = (ts // bin_seconds).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0913f22b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:30.534207Z",
     "start_time": "2025-11-06T06:18:30.511115Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "def compute_node_centralities_fast2(\n",
    "    df: pd.DataFrame,\n",
    "    ip2idx: dict,\n",
    "    src_col: str = \"Src IP\",\n",
    "    dst_col: str = \"Dst IP\",\n",
    "    use_betweenness: bool = False,     # set True if you have NetworKit\n",
    "    betw_samples: int = 32,            # NetworKit ApproxBetweenness samples\n",
    "    pagerank_alpha: float = 0.85,\n",
    "    pagerank_iters: int = 40,\n",
    "    pagerank_tol: float = 1e-6,\n",
    "    closeness_if_small: int = 20_000,  # only compute closeness if N <= this\n",
    "    ktruss_if_small: int = 10_000      # only compute k-truss if N <= this\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns C: [N_nodes, 5] -> [degree, betweenness, closeness, pagerank, ktruss_level], z-scored per column.\n",
    "    Fast path fills unavailable metrics with 0 (safe after z-scoring).\n",
    "    \"\"\"\n",
    "    N = len(ip2idx)\n",
    "    if N == 0:\n",
    "        return np.zeros((0,5), dtype=float)\n",
    "\n",
    "    # ------- Build sparse adjacency (directed) fast -------\n",
    "    # Map to indices without Python loops\n",
    "    src_idx = df[src_col].astype(str).map(ip2idx).to_numpy()\n",
    "    dst_idx = df[dst_col].astype(str).map(ip2idx).to_numpy()\n",
    "    mask = np.isfinite(src_idx) & np.isfinite(dst_idx)\n",
    "    src_idx = src_idx[mask].astype(np.int64, copy=False)\n",
    "    dst_idx = dst_idx[mask].astype(np.int64, copy=False)\n",
    "\n",
    "    # Remove self-loops once (optional)\n",
    "    non_self = src_idx != dst_idx\n",
    "    src_idx, dst_idx = src_idx[non_self], dst_idx[non_self]\n",
    "\n",
    "    data = np.ones_like(src_idx, dtype=np.float64)\n",
    "    A = sparse.coo_matrix((data, (src_idx, dst_idx)), shape=(N, N)).tocsr()\n",
    "\n",
    "    # ------- Degree (total degree for directed) -------\n",
    "    outdeg = A.getnnz(axis=1)          # rows\n",
    "    indeg  = A.getnnz(axis=0)          # cols\n",
    "    degree = (outdeg + indeg).astype(np.float64)\n",
    "\n",
    "    # ------- PageRank (power iteration on sparse) -------\n",
    "    # Row-normalize A^T equivalent: P^T @ pr\n",
    "    outdeg_safe = np.maximum(outdeg, 1)\n",
    "    Dinv = sparse.diags(1.0 / outdeg_safe)\n",
    "    P = Dinv @ A                       # row-stochastic (on rows)\n",
    "    pr = np.full(N, 1.0 / N, dtype=np.float64)\n",
    "    teleport = (1.0 - pagerank_alpha) / N\n",
    "    for _ in range(pagerank_iters):\n",
    "        pr_new = pagerank_alpha * (P.T @ pr) + teleport\n",
    "        if np.linalg.norm(pr_new - pr, 1) < pagerank_tol:\n",
    "            pr = pr_new\n",
    "            break\n",
    "        pr = pr_new\n",
    "\n",
    "    # ------- Betweenness (optional, NetworKit) -------\n",
    "    betw = np.zeros(N, dtype=np.float64)\n",
    "    if use_betweenness:\n",
    "        try:\n",
    "            import networkit as nk\n",
    "            # Build NetworKit graph\n",
    "            Gnk = nk.Graph(n=N, weighted=False, directed=True)\n",
    "            # Add edges (NetworKit expects int indices)\n",
    "            # Faster add: iterate CSR rows\n",
    "            rows, cols = A.nonzero()\n",
    "            for u, v in zip(rows.tolist(), cols.tolist()):\n",
    "                if u != v:\n",
    "                    Gnk.addEdge(u, v)\n",
    "            c = nk.centrality.ApproxBetweenness(Gnk, nSamples=int(betw_samples), normalized=True)\n",
    "            c.run()\n",
    "            betw = np.array(c.scores(), dtype=np.float64)\n",
    "        except Exception:\n",
    "            # If NetworKit not available, keep zeros (safe after z-score)\n",
    "            pass\n",
    "\n",
    "    # ------- Closeness (tiny graphs only, else zeros) -------\n",
    "    clos = np.zeros(N, dtype=np.float64)\n",
    "    if N <= closeness_if_small:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            H = nx.from_scipy_sparse_array(A, create_using=nx.DiGraph)\n",
    "            # Use NX fast approximation? (still Python; okay for small N)\n",
    "            clos_dict = nx.closeness_centrality(H)  # directed-version\n",
    "            # Map dict to array by index order\n",
    "            # NetworkX labels are 0..N-1 when built from scipy sparse\n",
    "            clos = np.array([clos_dict.get(i, 0.0) for i in range(N)], dtype=np.float64)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ------- k-truss (tiny graphs only, else zeros) -------\n",
    "    ktr = np.zeros(N, dtype=np.float64)\n",
    "    if N <= ktruss_if_small:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            Hu = nx.from_scipy_sparse_array((A + A.T).sign(), create_using=nx.Graph)\n",
    "            ktr_level = np.zeros(N, dtype=np.int32)\n",
    "            for k in range(3, 7):  # modest bound; raise carefully\n",
    "                try:\n",
    "                    Tk = nx.k_truss(Hu, k)\n",
    "                except nx.NetworkXError:\n",
    "                    break\n",
    "                nodes = list(Tk.nodes())\n",
    "                if not nodes:\n",
    "                    continue\n",
    "                ktr_level[np.array(nodes, dtype=np.int64)] = np.maximum(ktr_level[np.array(nodes, dtype=np.int64)], k)\n",
    "            ktr = ktr_level.astype(np.float64)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ------- Pack in the right slot order: [degree, betweenness, closeness, pagerank, ktruss] -------\n",
    "    C = np.column_stack([degree, betw, clos, pr, ktr])\n",
    "\n",
    "    # ------- z-score per column (robust to zeros) -------\n",
    "    mu = C.mean(axis=0, keepdims=True)\n",
    "    sd = C.std(axis=0, keepdims=True) + 1e-8\n",
    "    C = (C - mu) / sd\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27190bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:30.898765Z",
     "start_time": "2025-11-06T06:18:30.874841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fast unique IP mapping (string-safe)\n",
    "def make_ip_index(df, src_col='Src IP', dst_col='Dst IP'):\n",
    "    # Combine columns as one numpy array without concat copies\n",
    "    src = df[src_col].astype(str).to_numpy(copy=False)\n",
    "    dst = df[dst_col].astype(str).to_numpy(copy=False)\n",
    "    all_ips = np.concatenate((src, dst))\n",
    "\n",
    "    # Use pandas categorical (internally fast hash-based unique)\n",
    "    cat = pd.Categorical(all_ips)\n",
    "    ip2idx = dict(zip(cat.categories, range(len(cat.categories))))\n",
    "    n_nodes = len(ip2idx)\n",
    "    return ip2idx, n_nodes\n",
    "\n",
    "def build_snapshots(df, scaler_edge=None, fit_scaler=False, bin_seconds=300, device='cpu', include_per_bin_feats=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      snapshots: list[Data] in time order\n",
    "      ip2idx: dict mapping IP -> node index (per full dataset, stable across train/test)\n",
    "      scaler_edge: fitted StandardScaler for edge features\n",
    "      edge_cols_kept: list of columns used (existing + non-NA + time enc + centralities names)\n",
    "    \"\"\"\n",
    "    # Keep only available columns\n",
    "    edge_cols = [c for c in EDGE_COLS if c in df.columns]\n",
    "    cols_needed = ID_COLS + edge_cols + [LABEL_COL]\n",
    "    cols_needed = [c for c in cols_needed if c in df.columns]\n",
    "    df = df[cols_needed].dropna(subset=['Src IP','Dst IP'])\n",
    "    df = bin_time(df, bin_seconds=bin_seconds)\n",
    "\n",
    "    edge_cols = [c for c in EDGE_COLS if c in df.columns]\n",
    "    for c in edge_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df[edge_cols] = df[edge_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Edge feature scaler\n",
    "    if scaler_edge is None:\n",
    "        scaler_edge = StandardScaler()\n",
    "        fit_scaler = True\n",
    "    if fit_scaler and len(edge_cols) > 0:\n",
    "        scaler_edge.fit(df[edge_cols].astype(float).values)\n",
    "\n",
    "    ip2idx, n_nodes = make_ip_index(df)\n",
    "\n",
    "    # --- NEW: centralities over the full graph ---\n",
    "    C = compute_node_centralities_fast2( df, ip2idx,use_betweenness=False,     betw_samples=32,       pagerank_iters=40)\n",
    "\n",
    "    snapshots = []\n",
    "    prev_activity = defaultdict(int)  # lag-1 activity per node\n",
    "\n",
    "    # iterate bins\n",
    "    for b, g in df.sort_values('_bin').groupby('_bin'):\n",
    "        # Map nodes\n",
    "        src = g['Src IP'].map(ip2idx).astype(int).values\n",
    "        dst = g['Dst IP'].map(ip2idx).astype(int).values\n",
    "        edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
    "\n",
    "        # Edge attributes = scaled flow features + time encoding\n",
    "        if len(edge_cols) > 0:\n",
    "            eX = scaler_edge.transform(g[edge_cols].astype(float).values)\n",
    "        else:\n",
    "            eX = np.empty((len(g), 0), dtype=float)\n",
    "        # tfe = time_posenc(g['_epoch'].values)  # [E, 2*len(periods)]\n",
    "        # edge_attr_np = np.hstack([eX, tfe])\n",
    "        edge_attr_np = np.hstack([eX])\n",
    "        edge_attr = torch.tensor(edge_attr_np, dtype=torch.float)\n",
    "        edge_attr = torch.nan_to_num(edge_attr, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "        # Labels (edge-level)\n",
    "        y = torch.tensor(g[LABEL_COL].astype(int).values, dtype=torch.long)\n",
    "\n",
    "        # Node features:\n",
    "        #   (paper) centralities C; (yours) optional per-bin degrees + prev_activity\n",
    "        if include_per_bin_feats:\n",
    "            out_deg = np.bincount(src, minlength=n_nodes)\n",
    "            in_deg  = np.bincount(dst, minlength=n_nodes)\n",
    "            deg     = (out_deg + in_deg).reshape(-1,1)\n",
    "            node_feat = np.hstack([\n",
    "                in_deg.reshape(-1,1),\n",
    "                out_deg.reshape(-1,1),\n",
    "                deg,\n",
    "                np.array([prev_activity[i] for i in range(n_nodes)]).reshape(-1,1)\n",
    "            ])\n",
    "            node_feat = np.log1p(node_feat)\n",
    "            x_np = np.hstack([node_feat, C])  # [N, 4 + 5]\n",
    "        else:\n",
    "            x_np = C  # strict paper-style init\n",
    "\n",
    "        x = torch.tensor(x_np, dtype=torch.float)\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y\n",
    "        )\n",
    "        data._bin = int(b)\n",
    "        snapshots.append(data)\n",
    "\n",
    "        # Update prev_activity for next bin (count edges touched by node this bin)\n",
    "        touched = np.bincount(np.concatenate([src, dst]), minlength=n_nodes)\n",
    "        for i, c in enumerate(touched):\n",
    "            prev_activity[i] = int(c)\n",
    "\n",
    "    # Column names returned (only for info)\n",
    "    # time_cols = [f'time_{i}' for i in range(tfe.shape[1])]\n",
    "    # cent_cols = ['cent_degree','cent_betweenness','cent_closeness','cent_pagerank','cent_ktruss']\n",
    "    # edge_cols_used = edge_cols + time_cols + cent_cols\n",
    "\n",
    "    # time_cols = [f'time_{i}' for i in range(tfe.shape[1])]\n",
    "    cent_cols = ['cent_degree','cent_betweenness','cent_closeness','cent_pagerank','cent_ktruss']\n",
    "    edge_cols_used = edge_cols + cent_cols\n",
    "    return snapshots, ip2idx, scaler_edge, edge_cols_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6472af76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:31.040373Z",
     "start_time": "2025-11-06T06:18:31.029906Z"
    }
   },
   "outputs": [],
   "source": [
    "class EdgeGraphSAGEConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Edge-aware GraphSAGE (E-GraphSAGE-like):\n",
    "      m_ij = gate([x_i, x_j, e_ij]) * Ï†( Wj x_j + Wi x_i + We e_ij )\n",
    "      h_i' = Norm( mean_j m_ij + Wself x_i )     (residual)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, edge_in: int, out_channels: int, aggr: str = \"mean\", dropout: float = 0.0):\n",
    "        super().__init__(aggr=aggr, node_dim=0)\n",
    "        self.lin_src  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.lin_dst  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_in,    out_channels, bias=False)\n",
    "        self.lin_self = nn.Linear(in_channels, out_channels, bias=True)\n",
    "\n",
    "        # Small gate that decides how much of each message passes through\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(in_channels + in_channels + edge_in, out_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)  # -> [N, Fout]\n",
    "        out = out + self.lin_self(x)  # residual\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        msg_raw = self.lin_src(x_j) + self.lin_dst(x_i) + self.lin_edge(edge_attr)\n",
    "        g = torch.sigmoid(self.gate(torch.cat([x_i, x_j, edge_attr], dim=-1)))\n",
    "        msg = F.relu(msg_raw) * g\n",
    "        return self.dropout(msg)\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba02d20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:31.125970Z",
     "start_time": "2025-11-06T06:18:31.109081Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class TemporalEdgeSAGEClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE over each snapshot + GRUCell over node states across time.\n",
    "    Compatible with your EdgeGraphSAGEConv and edge-level head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_node: int, in_edge: int, hidden: int = 128, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden                    # <-- expose hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # --- spatial backbone (edge-aware GraphSAGE) ---\n",
    "        from torch_geometric.nn import MessagePassing\n",
    "\n",
    "        class EdgeGraphSAGEConv(MessagePassing):\n",
    "            def __init__(self, in_channels, edge_in, out_channels, aggr=\"mean\", dropout=0.0):\n",
    "                super().__init__(aggr=aggr, node_dim=0)\n",
    "                self.lin_src  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "                self.lin_dst  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "                self.lin_edge = nn.Linear(edge_in,    out_channels, bias=False)\n",
    "                self.lin_self = nn.Linear(in_channels, out_channels, bias=True)\n",
    "                self.gate = nn.Sequential(\n",
    "                    nn.Linear(in_channels + in_channels + edge_in, out_channels // 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(out_channels // 2, 1)\n",
    "                )\n",
    "                self.norm = nn.LayerNorm(out_channels)\n",
    "                self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "            def forward(self, x, edge_index, edge_attr):\n",
    "                out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "                out = out + self.lin_self(x)\n",
    "                out = self.norm(out)\n",
    "                return out\n",
    "\n",
    "            def message(self, x_i, x_j, edge_attr):\n",
    "                msg_raw = self.lin_src(x_j) + self.lin_dst(x_i) + self.lin_edge(edge_attr)\n",
    "                g = torch.sigmoid(self.gate(torch.cat([x_i, x_j, edge_attr], dim=-1)))\n",
    "                msg = F.relu(msg_raw) * g\n",
    "                return self.dropout(msg)\n",
    "\n",
    "            def update(self, aggr_out):\n",
    "                return aggr_out\n",
    "\n",
    "        layers = []\n",
    "        dims = [in_node] + [hidden] * num_layers\n",
    "        for i in range(num_layers):\n",
    "            layers.append(EdgeGraphSAGEConv(dims[i], in_edge, dims[i+1], aggr=\"mean\", dropout=dropout))\n",
    "        self.convs = nn.ModuleList(layers)\n",
    "\n",
    "        # --- temporal cell over nodes ---\n",
    "        self.gru = nn.GRUCell(hidden, hidden)\n",
    "\n",
    "        # --- edge head (uses recurrent node states) ---\n",
    "        edge_head_in = (2 * hidden) + in_edge + hidden + hidden  # [h_s, h_d, e, |h_s-h_d|, h_s*h_d]\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_head_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 2)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_state(self, num_nodes: int, device=None):\n",
    "        \"\"\"Create zero initial node states of shape [num_nodes, hidden].\"\"\"\n",
    "        return torch.zeros(num_nodes, self.hidden, device=device)\n",
    "\n",
    "    def spatial_encode(self, data: Data) -> torch.Tensor:\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        return x  # [N, hidden]\n",
    "\n",
    "    def forward(self, data: Data, h_prev: torch.Tensor | None = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns (edge_logits, h_t)\n",
    "        \"\"\"\n",
    "        x_star = self.spatial_encode(data)                     # [N, H]\n",
    "        if h_prev is None:\n",
    "            h_prev = x_star.new_zeros(x_star.size(0), x_star.size(1))\n",
    "        h_t = self.gru(x_star, h_prev)                         # [N, H]\n",
    "\n",
    "        src, dst = data.edge_index\n",
    "        h_s, h_d = h_t[src], h_t[dst]\n",
    "        h_abs = torch.abs(h_s - h_d)\n",
    "        h_mul = h_s * h_d\n",
    "        z = torch.cat([h_s, h_d, data.edge_attr, h_abs, h_mul], dim=-1)\n",
    "        logits = self.edge_mlp(z)\n",
    "        return logits, h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4fcd6eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:31.177029Z",
     "start_time": "2025-11-06T06:18:31.169193Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_epoch_fullgraph(model, snapshots, optimizer=None, device='cpu'):\n",
    "    is_train = optimizer is not None\n",
    "    total_loss, total_correct, total_edges = 0.0, 0, 0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    for data in snapshots:\n",
    "        data = data.to(device)\n",
    "        logits = model(data)\n",
    "        if is_train:\n",
    "            loss = ce(logits, data.y)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item()) * data.y.numel()\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_trues.append(data.y.cpu().numpy())\n",
    "            total_correct += int((pred == data.y).sum())\n",
    "            total_edges += int(data.y.numel())\n",
    "\n",
    "    # Metrics\n",
    "    if all_trues:\n",
    "        y_true = np.concatenate(all_trues)\n",
    "        y_pred = np.concatenate(all_preds)\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        # FPR: FP / N_negatives\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)\n",
    "        fpr = fp / max(1, (fp + tn))\n",
    "    else:\n",
    "        weighted_f1, fpr = float('nan'), float('nan')\n",
    "\n",
    "    avg_loss = (total_loss / max(1, total_edges)) if is_train else None\n",
    "    acc = total_correct / max(1, total_edges)\n",
    "    return avg_loss, acc, weighted_f1, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8e70cb68d10632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:09:09.074024Z",
     "start_time": "2025-11-06T06:09:09.057718Z"
    }
   },
   "outputs": [],
   "source": [
    "class AsymmetricLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=2.0, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gp, self.gn, self.eps = gamma_pos, gamma_neg, eps\n",
    "    def forward(self, logits, y):\n",
    "        p = torch.softmax(logits, dim=1)[:,1]\n",
    "        yf = y.float()\n",
    "        pt = p*yf + (1-p)*(1-yf)\n",
    "        gamma = self.gp*yf + self.gn*(1-yf)\n",
    "        loss = - (yf*torch.log(p+self.eps) + (1-yf)*torch.log(1-p+self.eps)) * ((1-pt)**gamma)\n",
    "        return loss.mean()\n",
    "\n",
    "# in training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2a7676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:31.225062Z",
     "start_time": "2025-11-06T06:18:31.209276Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "def run_epoch_neighbor_temporal(\n",
    "    model,\n",
    "    snapshots,\n",
    "    optimizer=None,\n",
    "    loss_fn=None,\n",
    "    device='cuda',\n",
    "    num_neighbors=[25, 10],\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    "    clip=2.0,\n",
    "    tbptt=5,                 # detach global state every N mini-batches to cap memory\n",
    "):\n",
    "    \"\"\"\n",
    "    If optimizer is None -> eval mode (no loss/updates, returns avg_loss=None).\n",
    "    Maintains a global H over nodes; per mini-batch we read/write H[batch.n_id].\n",
    "    \"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    # All snapshots in a split share the same node index space\n",
    "    N = snapshots[0].x.size(0)\n",
    "    H_global = torch.zeros(N, model.hidden, device=device)\n",
    "\n",
    "    total_loss, total_correct, total_edges = 0.0, 0, 0\n",
    "    all_preds, all_trues = [], []\n",
    "    steps_since_detach = 0\n",
    "\n",
    "    # Process snapshots in chronological order\n",
    "    for snap in sorted(snapshots, key=lambda d: getattr(d, \"_bin\", 0)):\n",
    "        loader = NeighborLoader(\n",
    "            snap, num_neighbors=num_neighbors, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Map global node states -> batch node states\n",
    "            h_prev_batch = H_global[batch.n_id]                         # [N_batch_nodes, H]\n",
    "            logits, h_t_batch = model(batch, h_prev=h_prev_batch)       # (edge_logits, new_node_states)\n",
    "\n",
    "            if is_train:\n",
    "                loss = loss_fn(logits, batch.y)\n",
    "                if torch.isfinite(loss):\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                    optimizer.step()\n",
    "                    total_loss += float(loss.item()) * batch.y.numel()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(1)\n",
    "                all_preds.append(pred.detach().cpu().numpy())\n",
    "                all_trues.append(batch.y.detach().cpu().numpy())\n",
    "                total_correct += int((pred == batch.y).sum())\n",
    "                total_edges   += int(batch.y.numel())\n",
    "\n",
    "                # Write back updated node states for nodes seen in this subgraph\n",
    "                H_global[batch.n_id] = h_t_batch.detach()\n",
    "\n",
    "            # Truncated BPTT: periodically detach the whole global state\n",
    "            steps_since_detach += 1\n",
    "            if is_train and steps_since_detach >= tbptt:\n",
    "                H_global = H_global.detach()\n",
    "                steps_since_detach = 0\n",
    "\n",
    "    # Metrics\n",
    "    if all_trues:\n",
    "        y_true = np.concatenate(all_trues)\n",
    "        y_pred = np.concatenate(all_preds)\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        fpr = fp / max(1, (fp + tn))\n",
    "        acc = (y_pred == y_true).mean()\n",
    "    else:\n",
    "        weighted_f1, fpr, acc = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    avg_loss = (total_loss / max(1, total_edges)) if is_train else None\n",
    "    return avg_loss, acc, weighted_f1, fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f867f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:19:54.854673Z",
     "start_time": "2025-11-06T06:18:58.813746Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('data/data_gml/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f409b326a15f9232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:18:58.788007Z",
     "start_time": "2025-11-06T06:18:45.731010Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv('data/data_gml/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1981f38243bd087e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:19:54.914078Z",
     "start_time": "2025-11-06T06:19:54.897496Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_numeric_dtypes(df: pd.DataFrame, try_float16: bool = False, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to the smallest possible dtype without changing values.\n",
    "    - Integers: downcast to smallest signed/unsigned integer.\n",
    "    - Floats: downcast to float32 (and optionally float16 if lossless within tolerance).\n",
    "    Returns a new DataFrame (original unchanged).\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    start_mem = result.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    num_cols = [c for c in result.columns if pd.api.types.is_numeric_dtype(result[c])]\n",
    "    for c in num_cols:\n",
    "        col = result[c]\n",
    "\n",
    "        # Skip all-NaN\n",
    "        if col.notnull().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            # Integer (no NaNs)\n",
    "            if col.min() >= 0:\n",
    "                result[c] = pd.to_numeric(col, downcast=\"unsigned\")\n",
    "            else:\n",
    "                result[c] = pd.to_numeric(col, downcast=\"integer\")\n",
    "\n",
    "        elif pd.api.types.is_float_dtype(col):\n",
    "            # First, try float32\n",
    "            col32 = col.astype(np.float32)\n",
    "            if np.allclose(col.values, col32.values, equal_nan=True):\n",
    "                result[c] = col32\n",
    "                # Optionally try float16 (more aggressive)\n",
    "                if try_float16:\n",
    "                    col16 = col.astype(np.float16)\n",
    "                    if np.allclose(col.values, col16.astype(np.float32).values, rtol=1e-03, atol=1e-06, equal_nan=True):\n",
    "                        result[c] = col16\n",
    "            # else keep original float64\n",
    "\n",
    "        # If it's a nullable integer (Int64/Int32), try to preserve nulls with the smallest nullable int\n",
    "        elif pd.api.types.is_dtype_equal(col.dtype, \"Int64\") or str(col.dtype).startswith(\"Int\"):\n",
    "            if col.min() >= 0:\n",
    "                tmp = pd.to_numeric(col.astype(\"float64\"), downcast=\"unsigned\")\n",
    "            else:\n",
    "                tmp = pd.to_numeric(col.astype(\"float64\"), downcast=\"integer\")\n",
    "            # Cast back to nullable integer if still integer-like\n",
    "            if pd.api.types.is_integer_dtype(tmp):\n",
    "                result[c] = pd.Series(tmp, index=col.index).astype(pd.ArrowDtype(tmp.dtype.name) if hasattr(pd, \"ArrowDtype\") else tmp.dtype)\n",
    "\n",
    "    end_mem = result.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory: {start_mem:.2f} MB â†’ {end_mem:.2f} MB ({(start_mem-end_mem):.2f} MB saved, {(1 - end_mem/max(start_mem,1e-9))*100:.1f}% reduction)\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- Example ---\n",
    "# df_optimized = optimize_numeric_dtypes(df, try_float16=False, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6f7cccec9383cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:20:36.226891Z",
     "start_time": "2025-11-06T06:19:55.000035Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df_small = optimize_numeric_dtypes(train_df.copy(), True, True)\n",
    "# test_df_small = optimize_numeric_dtypes(test_df.copy(), True, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d30bbd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:20:36.332724Z",
     "start_time": "2025-11-06T06:20:36.323362Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'mps' if torch.backends.mps.is_available() else device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162787adc5e6f26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:20:36.401048Z",
     "start_time": "2025-11-06T06:20:36.387079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "174984bbd266c9b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:25:06.383698Z",
     "start_time": "2025-11-06T06:22:14.339088Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Build train snapshots (fit scaler)\n",
    "# train_snaps, train_ip2idx, scaler_edge, edge_cols_used = build_snapshots(\n",
    "#     train_df_small, scaler_edge=None, fit_scaler=True, bin_seconds=300, device=device, include_per_bin_feats=False\n",
    "# )\n",
    "\n",
    "# # Build test snapshots (reuse scaler; separate ip2idx for strict inductive)\n",
    "# test_snaps, test_ip2idx, _, _ = build_snapshots(\n",
    "#     test_df_small, scaler_edge=scaler_edge, fit_scaler=False, bin_seconds=300, device=device, include_per_bin_feats=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3736673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/snapshots_data.pkl\", \"rb\") as f:\n",
    "    saved = pickle.load(f)\n",
    "\n",
    "train_snaps = saved[\"train_snaps\"]\n",
    "test_snaps = saved[\"test_snaps\"]\n",
    "scaler_edge = saved[\"scaler_edge\"]\n",
    "train_ip2idx = saved[\"train_ip2idx\"]\n",
    "test_ip2idx = saved[\"test_ip2idx\"]\n",
    "edge_cols_used = saved[\"edge_cols_used\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b8b4853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:25:12.886487Z",
     "start_time": "2025-11-06T06:25:11.778523Z"
    }
   },
   "outputs": [],
   "source": [
    "in_node = train_snaps[0].x.size(1)         # now includes centralities (+ optional per-bin feats)\n",
    "in_edge = train_snaps[0].edge_attr.size(1) # edge features + time enc\n",
    "model = TemporalEdgeSAGEClassifier(in_node=in_node, in_edge=in_edge,\n",
    "                                hidden=32, num_layers=2, dropout=0.2).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59306c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_neighborhood_sampling():\n",
    "    device = 'mps'\n",
    "    results = {}\n",
    "\n",
    "    in_node = train_snaps[0].x.size(1) \n",
    "    in_edge = train_snaps[0].edge_attr.size(1)\n",
    "    model = TemporalEdgeSAGEClassifier(in_node=in_node, in_edge=in_edge,\n",
    "                                    hidden=32, num_layers=2, dropout=0.2).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    loss_fns = [nn.CrossEntropyLoss(), AsymmetricLoss(gamma_pos=0.0, gamma_neg=2.0)]\n",
    "\n",
    "    EPOCHS = 3\n",
    "    for loss_fn in loss_fns:\n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            # Train across ALL train snapshots (temporal state is carried within the function)\n",
    "            tr_loss, tr_acc, tr_f1, tr_fpr = run_epoch_neighbor_temporal(\n",
    "                model, train_snaps, optimizer=opt, device=device,\n",
    "                num_neighbors=[25,10], batch_size=4096, shuffle=True, tbptt=5\n",
    "            )\n",
    "\n",
    "            # Evaluate across ALL test snapshots (no optimizer = eval)\n",
    "            _, te_acc, te_f1, te_fpr = run_epoch_neighbor_temporal(\n",
    "                model, test_snaps, optimizer=None, device=device,\n",
    "                num_neighbors=[25,10], batch_size=4096, shuffle=False, tbptt=5\n",
    "            )\n",
    "            results[(loss_fn.__class__.__name__, epoch)] = {\n",
    "                'train_loss': tr_loss,\n",
    "                'train_acc': tr_acc,\n",
    "                'train_f1': tr_f1,\n",
    "                'train_fpr': tr_fpr,\n",
    "                'test_acc': te_acc,\n",
    "                'test_f1': te_f1,\n",
    "                'test_fpr': te_fpr\n",
    "            }\n",
    "            print(f\"Using loss function: {loss_fn.__class__.__name__}\")\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train loss {tr_loss:.4f} | train acc {tr_acc:.4f} | \"\n",
    "                f\"train F1 {tr_f1:.4f} | train FPR {tr_fpr:.4f} | \"\n",
    "                f\"test acc {te_acc:.4f} | test F1 {te_f1:.4f} | test FPR {te_fpr:.4f}\"\n",
    "            )\n",
    "    return results\n",
    "baseline_results = experiment_neighborhood_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8666e2aec4079f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:54:45.603853Z",
     "start_time": "2025-11-06T06:54:45.593153Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "@torch.no_grad()\n",
    "def confusion_and_plot_temporal(model, snapshots, device='cuda', labels=(0, 1), title='Confusion Matrix (Test)'):\n",
    "    \"\"\"\n",
    "    Evaluate a temporal GNN across snapshots in chronological order,\n",
    "    carrying node state across time. Plots and returns the confusion matrix.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Sort by time and init recurrent state\n",
    "    snaps = sorted(snapshots, key=lambda d: getattr(d, \"_bin\", 0))\n",
    "    N0 = snaps[0].x.size(0)\n",
    "    hidden_dim = getattr(model, \"hidden\", None) or model.gru.hidden_size\n",
    "    H = torch.zeros(N0, hidden_dim, device=device)\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for snap in snaps:\n",
    "        snap = snap.to(device)\n",
    "\n",
    "        # If node count differs, re-init state (e.g., different split)\n",
    "        if H.size(0) != snap.x.size(0):\n",
    "            H = torch.zeros(snap.x.size(0), hidden_dim, device=device)\n",
    "\n",
    "        logits, H = model(snap, H)  # temporal forward\n",
    "        preds  = logits.argmax(dim=1).cpu().numpy()\n",
    "        labels_np = snap.y.cpu().numpy()\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels_np)\n",
    "\n",
    "        H = H.detach()  # truncate (safety)\n",
    "\n",
    "    y_pred = np.concatenate(all_preds) if all_preds else np.array([])\n",
    "    y_true = np.concatenate(all_labels) if all_labels else np.array([])\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(labels))\n",
    "    report = classification_report(y_true, y_pred, labels=list(labels), digits=4, zero_division=0)\n",
    "\n",
    "    # --- Plot above the text output ---\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=[f'Pred {l}' for l in labels],\n",
    "        yticklabels=[f'True {l}' for l in labels]\n",
    "    )\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ðŸ“Š Classification Report:\")\n",
    "    print(report)\n",
    "    return cm, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a756ab748ec0907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:54:53.266994Z",
     "start_time": "2025-11-06T06:54:49.410461Z"
    }
   },
   "outputs": [],
   "source": [
    "cm, report = confusion_and_plot_temporal(model, test_snaps, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
