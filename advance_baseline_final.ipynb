{
 "cells": [
  {
   "cell_type": "code",
   "id": "ff190f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:12.978105Z",
     "start_time": "2025-11-02T13:47:05.562289Z"
    }
   },
   "source": [
    "import os, math, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pandas import read_csv\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import networkx as nx"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e2d61ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:12.996363Z",
     "start_time": "2025-11-02T13:47:12.989878Z"
    }
   },
   "source": [
    "\n",
    "EDGE_COLS = [\n",
    "    'Bwd Packet Length Min', 'Protocol_6', 'Bwd Packets/s', 'FWD Init Win Bytes',\n",
    "    'Packet Length Std', 'FIN Flag Count', 'SrcPortRange_registered',\n",
    "    'Packet Length Min', 'Fwd Seg Size Min', 'DstPortRange_well_known',\n",
    "    'Bwd IAT Total', 'SYN Flag Count', 'Bwd Packet Length Std'\n",
    "]\n",
    "ID_COLS = ['Src IP','Dst IP','Timestamp']\n",
    "LABEL_COL = 'target'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3662ee25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:13.019443Z",
     "start_time": "2025-11-02T13:47:13.009816Z"
    }
   },
   "source": [
    "# def time_posenc(t, periods=(60, 300, 3600)):\n",
    "#     # t: numpy array of epoch seconds\n",
    "#     feats = []\n",
    "#     for P in periods:\n",
    "#         w = 2*math.pi/P\n",
    "#         feats.append(np.sin(w*t))\n",
    "#         feats.append(np.cos(w*t))\n",
    "#     return np.stack(feats, axis=1)  # [N, 2*len(periods)]\n",
    "\n",
    "def bin_time(df, bin_seconds=300):\n",
    "    # Expect df['Timestamp'] as datetime or string; convert to seconds\n",
    "    ts = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True).astype('int64') // 10**9\n",
    "    df = df.copy()\n",
    "    df['_epoch'] = ts\n",
    "    df['_bin'] = (ts // bin_seconds).astype(int)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "0913f22b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:13.037558Z",
     "start_time": "2025-11-02T13:47:13.024573Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "def compute_node_centralities_fast2(\n",
    "    df: pd.DataFrame,\n",
    "    ip2idx: dict,\n",
    "    src_col: str = \"Src IP\",\n",
    "    dst_col: str = \"Dst IP\",\n",
    "    use_betweenness: bool = False,     # set True if you have NetworKit\n",
    "    betw_samples: int = 32,            # NetworKit ApproxBetweenness samples\n",
    "    pagerank_alpha: float = 0.85,\n",
    "    pagerank_iters: int = 40,\n",
    "    pagerank_tol: float = 1e-6,\n",
    "    closeness_if_small: int = 20_000,  # only compute closeness if N <= this\n",
    "    ktruss_if_small: int = 10_000      # only compute k-truss if N <= this\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns C: [N_nodes, 5] -> [degree, betweenness, closeness, pagerank, ktruss_level], z-scored per column.\n",
    "    Fast path fills unavailable metrics with 0 (safe after z-scoring).\n",
    "    \"\"\"\n",
    "    N = len(ip2idx)\n",
    "    if N == 0:\n",
    "        return np.zeros((0,5), dtype=float)\n",
    "\n",
    "    # ------- Build sparse adjacency (directed) fast -------\n",
    "    # Map to indices without Python loops\n",
    "    src_idx = df[src_col].astype(str).map(ip2idx).to_numpy()\n",
    "    dst_idx = df[dst_col].astype(str).map(ip2idx).to_numpy()\n",
    "    mask = np.isfinite(src_idx) & np.isfinite(dst_idx)\n",
    "    src_idx = src_idx[mask].astype(np.int64, copy=False)\n",
    "    dst_idx = dst_idx[mask].astype(np.int64, copy=False)\n",
    "\n",
    "    # Remove self-loops once (optional)\n",
    "    non_self = src_idx != dst_idx\n",
    "    src_idx, dst_idx = src_idx[non_self], dst_idx[non_self]\n",
    "\n",
    "    data = np.ones_like(src_idx, dtype=np.float64)\n",
    "    A = sparse.coo_matrix((data, (src_idx, dst_idx)), shape=(N, N)).tocsr()\n",
    "\n",
    "    # ------- Degree (total degree for directed) -------\n",
    "    outdeg = A.getnnz(axis=1)          # rows\n",
    "    indeg  = A.getnnz(axis=0)          # cols\n",
    "    degree = (outdeg + indeg).astype(np.float64)\n",
    "\n",
    "    # ------- PageRank (power iteration on sparse) -------\n",
    "    # Row-normalize A^T equivalent: P^T @ pr\n",
    "    outdeg_safe = np.maximum(outdeg, 1)\n",
    "    Dinv = sparse.diags(1.0 / outdeg_safe)\n",
    "    P = Dinv @ A                       # row-stochastic (on rows)\n",
    "    pr = np.full(N, 1.0 / N, dtype=np.float64)\n",
    "    teleport = (1.0 - pagerank_alpha) / N\n",
    "    for _ in range(pagerank_iters):\n",
    "        pr_new = pagerank_alpha * (P.T @ pr) + teleport\n",
    "        if np.linalg.norm(pr_new - pr, 1) < pagerank_tol:\n",
    "            pr = pr_new\n",
    "            break\n",
    "        pr = pr_new\n",
    "\n",
    "    # ------- Betweenness (optional, NetworKit) -------\n",
    "    betw = np.zeros(N, dtype=np.float64)\n",
    "    if use_betweenness:\n",
    "        try:\n",
    "            import networkit as nk\n",
    "            # Build NetworKit graph\n",
    "            Gnk = nk.Graph(n=N, weighted=False, directed=True)\n",
    "            # Add edges (NetworKit expects int indices)\n",
    "            # Faster add: iterate CSR rows\n",
    "            rows, cols = A.nonzero()\n",
    "            for u, v in zip(rows.tolist(), cols.tolist()):\n",
    "                if u != v:\n",
    "                    Gnk.addEdge(u, v)\n",
    "            c = nk.centrality.ApproxBetweenness(Gnk, nSamples=int(betw_samples), normalized=True)\n",
    "            c.run()\n",
    "            betw = np.array(c.scores(), dtype=np.float64)\n",
    "        except Exception:\n",
    "            # If NetworKit not available, keep zeros (safe after z-score)\n",
    "            pass\n",
    "\n",
    "    # ------- Closeness (tiny graphs only, else zeros) -------\n",
    "    clos = np.zeros(N, dtype=np.float64)\n",
    "    if N <= closeness_if_small:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            H = nx.from_scipy_sparse_array(A, create_using=nx.DiGraph)\n",
    "            # Use NX fast approximation? (still Python; okay for small N)\n",
    "            clos_dict = nx.closeness_centrality(H)  # directed-version\n",
    "            # Map dict to array by index order\n",
    "            # NetworkX labels are 0..N-1 when built from scipy sparse\n",
    "            clos = np.array([clos_dict.get(i, 0.0) for i in range(N)], dtype=np.float64)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ------- k-truss (tiny graphs only, else zeros) -------\n",
    "    ktr = np.zeros(N, dtype=np.float64)\n",
    "    if N <= ktruss_if_small:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            Hu = nx.from_scipy_sparse_array((A + A.T).sign(), create_using=nx.Graph)\n",
    "            ktr_level = np.zeros(N, dtype=np.int32)\n",
    "            for k in range(3, 7):  # modest bound; raise carefully\n",
    "                try:\n",
    "                    Tk = nx.k_truss(Hu, k)\n",
    "                except nx.NetworkXError:\n",
    "                    break\n",
    "                nodes = list(Tk.nodes())\n",
    "                if not nodes:\n",
    "                    continue\n",
    "                ktr_level[np.array(nodes, dtype=np.int64)] = np.maximum(ktr_level[np.array(nodes, dtype=np.int64)], k)\n",
    "            ktr = ktr_level.astype(np.float64)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ------- Pack in the right slot order: [degree, betweenness, closeness, pagerank, ktruss] -------\n",
    "    C = np.column_stack([degree, betw, clos, pr, ktr])\n",
    "\n",
    "    # ------- z-score per column (robust to zeros) -------\n",
    "    mu = C.mean(axis=0, keepdims=True)\n",
    "    sd = C.std(axis=0, keepdims=True) + 1e-8\n",
    "    C = (C - mu) / sd\n",
    "    return C\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "27190bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:14.481990Z",
     "start_time": "2025-11-02T13:47:14.467302Z"
    }
   },
   "source": [
    "# Fast unique IP mapping (string-safe)\n",
    "def make_ip_index(df, src_col='Src IP', dst_col='Dst IP'):\n",
    "    # Combine columns as one numpy array without concat copies\n",
    "    src = df[src_col].astype(str).to_numpy(copy=False)\n",
    "    dst = df[dst_col].astype(str).to_numpy(copy=False)\n",
    "    all_ips = np.concatenate((src, dst))\n",
    "\n",
    "    # Use pandas categorical (internally fast hash-based unique)\n",
    "    cat = pd.Categorical(all_ips)\n",
    "    ip2idx = dict(zip(cat.categories, range(len(cat.categories))))\n",
    "    n_nodes = len(ip2idx)\n",
    "    return ip2idx, n_nodes\n",
    "\n",
    "def build_snapshots(df, scaler_edge=None, fit_scaler=False, bin_seconds=300, device='cpu', include_per_bin_feats=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      snapshots: list[Data] in time order\n",
    "      ip2idx: dict mapping IP -> node index (per full dataset, stable across train/test)\n",
    "      scaler_edge: fitted StandardScaler for edge features\n",
    "      edge_cols_kept: list of columns used (existing + non-NA + time enc + centralities names)\n",
    "    \"\"\"\n",
    "    # Keep only available columns\n",
    "    edge_cols = [c for c in EDGE_COLS if c in df.columns]\n",
    "    cols_needed = ID_COLS + edge_cols + [LABEL_COL]\n",
    "    cols_needed = [c for c in cols_needed if c in df.columns]\n",
    "    df = df[cols_needed].dropna(subset=['Src IP','Dst IP'])\n",
    "    df = bin_time(df, bin_seconds=bin_seconds)\n",
    "\n",
    "    edge_cols = [c for c in EDGE_COLS if c in df.columns]\n",
    "    for c in edge_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df[edge_cols] = df[edge_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Edge feature scaler\n",
    "    if scaler_edge is None:\n",
    "        scaler_edge = StandardScaler()\n",
    "        fit_scaler = True\n",
    "    if fit_scaler and len(edge_cols) > 0:\n",
    "        scaler_edge.fit(df[edge_cols].astype(float).values)\n",
    "\n",
    "    ip2idx, n_nodes = make_ip_index(df)\n",
    "\n",
    "    # --- NEW: centralities over the full graph ---\n",
    "    C = compute_node_centralities_fast2( df, ip2idx,use_betweenness=False,     betw_samples=32,       pagerank_iters=40)\n",
    "\n",
    "    snapshots = []\n",
    "    prev_activity = defaultdict(int)  # lag-1 activity per node\n",
    "\n",
    "    # iterate bins\n",
    "    for b, g in df.sort_values('_bin').groupby('_bin'):\n",
    "        # Map nodes\n",
    "        src = g['Src IP'].map(ip2idx).astype(int).values\n",
    "        dst = g['Dst IP'].map(ip2idx).astype(int).values\n",
    "        edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
    "\n",
    "        # Edge attributes = scaled flow features + time encoding\n",
    "        if len(edge_cols) > 0:\n",
    "            eX = scaler_edge.transform(g[edge_cols].astype(float).values)\n",
    "        else:\n",
    "            eX = np.empty((len(g), 0), dtype=float)\n",
    "        # tfe = time_posenc(g['_epoch'].values)  # [E, 2*len(periods)]\n",
    "        # edge_attr_np = np.hstack([eX, tfe])\n",
    "        edge_attr_np = np.hstack([eX])\n",
    "        edge_attr = torch.tensor(edge_attr_np, dtype=torch.float)\n",
    "        edge_attr = torch.nan_to_num(edge_attr, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "        # Labels (edge-level)\n",
    "        y = torch.tensor(g[LABEL_COL].astype(int).values, dtype=torch.long)\n",
    "\n",
    "        # Node features:\n",
    "        #   (paper) centralities C; (yours) optional per-bin degrees + prev_activity\n",
    "        if include_per_bin_feats:\n",
    "            out_deg = np.bincount(src, minlength=n_nodes)\n",
    "            in_deg  = np.bincount(dst, minlength=n_nodes)\n",
    "            deg     = (out_deg + in_deg).reshape(-1,1)\n",
    "            node_feat = np.hstack([\n",
    "                in_deg.reshape(-1,1),\n",
    "                out_deg.reshape(-1,1),\n",
    "                deg,\n",
    "                np.array([prev_activity[i] for i in range(n_nodes)]).reshape(-1,1)\n",
    "            ])\n",
    "            node_feat = np.log1p(node_feat)\n",
    "            x_np = np.hstack([node_feat, C])  # [N, 4 + 5]\n",
    "        else:\n",
    "            x_np = C  # strict paper-style init\n",
    "\n",
    "        x = torch.tensor(x_np, dtype=torch.float)\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y\n",
    "        )\n",
    "        data._bin = int(b)\n",
    "        snapshots.append(data)\n",
    "\n",
    "        # Update prev_activity for next bin (count edges touched by node this bin)\n",
    "        touched = np.bincount(np.concatenate([src, dst]), minlength=n_nodes)\n",
    "        for i, c in enumerate(touched):\n",
    "            prev_activity[i] = int(c)\n",
    "\n",
    "    # Column names returned (only for info)\n",
    "    # time_cols = [f'time_{i}' for i in range(tfe.shape[1])]\n",
    "    # cent_cols = ['cent_degree','cent_betweenness','cent_closeness','cent_pagerank','cent_ktruss']\n",
    "    # edge_cols_used = edge_cols + time_cols + cent_cols\n",
    "\n",
    "    # time_cols = [f'time_{i}' for i in range(tfe.shape[1])]\n",
    "    cent_cols = ['cent_degree','cent_betweenness','cent_closeness','cent_pagerank','cent_ktruss']\n",
    "    edge_cols_used = edge_cols + cent_cols\n",
    "    return snapshots, ip2idx, scaler_edge, edge_cols_used"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6472af76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:14.581938Z",
     "start_time": "2025-11-02T13:47:14.574243Z"
    }
   },
   "source": [
    "class EdgeGraphSAGEConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Edge-aware GraphSAGE (E-GraphSAGE-like):\n",
    "      m_ij = gate([x_i, x_j, e_ij]) * Ï†( Wj x_j + Wi x_i + We e_ij )\n",
    "      h_i' = Norm( mean_j m_ij + Wself x_i )     (residual)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, edge_in: int, out_channels: int, aggr: str = \"mean\", dropout: float = 0.0):\n",
    "        super().__init__(aggr=aggr, node_dim=0)\n",
    "        self.lin_src  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.lin_dst  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_in,    out_channels, bias=False)\n",
    "        self.lin_self = nn.Linear(in_channels, out_channels, bias=True)\n",
    "\n",
    "        # Small gate that decides how much of each message passes through\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(in_channels + in_channels + edge_in, out_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)  # -> [N, Fout]\n",
    "        out = out + self.lin_self(x)  # residual\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        msg_raw = self.lin_src(x_j) + self.lin_dst(x_i) + self.lin_edge(edge_attr)\n",
    "        g = torch.sigmoid(self.gate(torch.cat([x_i, x_j, edge_attr], dim=-1)))\n",
    "        msg = F.relu(msg_raw) * g\n",
    "        return self.dropout(msg)\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:\n",
    "        return aggr_out"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ba02d20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T14:02:22.923938Z",
     "start_time": "2025-11-02T14:02:22.917215Z"
    }
   },
   "source": [
    "class GraphTimeEdgeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    E-GraphSAGE-style backbone that uses edge attributes inside message passing\n",
    "    AND in the final edge-level head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_node: int, in_edge: int, hidden: int = 100, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        layers = []\n",
    "        dims = [in_node] + [hidden] * num_layers\n",
    "        for i in range(num_layers):\n",
    "            layers.append(EdgeGraphSAGEConv(dims[i], in_edge, dims[i+1], aggr=\"mean\", dropout=dropout))\n",
    "        self.convs = nn.ModuleList(layers)\n",
    "\n",
    "        # Edge MLP head: combine node embeddings and edge features\n",
    "        edge_head_in = (2 * hidden) + in_edge + hidden + hidden  # [h_s, h_d, e, |h_s-h_d|, h_s*h_d]\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_head_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        src, dst = edge_index\n",
    "        h_s, h_d = x[src], x[dst]\n",
    "        h_abs = torch.abs(h_s - h_d)\n",
    "        h_mul = h_s * h_d\n",
    "        z = torch.cat([h_s, h_d, edge_attr, h_abs, h_mul], dim=-1)\n",
    "\n",
    "        logits = self.edge_mlp(z)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "1a2a7676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:17.812080Z",
     "start_time": "2025-11-02T13:47:17.801910Z"
    }
   },
   "source": [
    "def run_epoch_neighbor(model, snapshot: Data, optimizer=None, device='cuda',\n",
    "                       num_neighbors=[25,10], batch_size=4096, shuffle=True):\n",
    "    is_train = optimizer is not None\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total_loss, total_correct, total_edges = 0.0, 0, 0\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    loader = NeighborLoader(snapshot, num_neighbors=num_neighbors, batch_size=batch_size, shuffle=shuffle)\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "\n",
    "        if is_train:\n",
    "            loss = ce(logits, batch.y)\n",
    "            if not torch.isfinite(loss):\n",
    "                # skip this minibatch if something went bad\n",
    "                continue\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item()) * batch.y.numel()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_trues.append(batch.y.cpu().numpy())\n",
    "            total_correct += int((pred == batch.y).sum())\n",
    "            total_edges += int(batch.y.numel())\n",
    "\n",
    "    # Metrics\n",
    "    if all_trues:\n",
    "        y_true = np.concatenate(all_trues)\n",
    "        y_pred = np.concatenate(all_preds)\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)\n",
    "        fpr = fp / max(1, (fp + tn))\n",
    "    else:\n",
    "        weighted_f1, fpr = float('nan'), float('nan')\n",
    "\n",
    "    avg_loss = (total_loss / max(1, total_edges)) if is_train else None\n",
    "    acc = total_correct / max(1, total_edges)\n",
    "    return avg_loss, acc, weighted_f1, fpr"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3f867f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:47:55.105473Z",
     "start_time": "2025-11-02T13:47:20.052383Z"
    }
   },
   "source": "train_df = pd.read_csv('train.csv')\n",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:48:11.357920Z",
     "start_time": "2025-11-02T13:47:55.108570Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = pd.read_csv('test_new.csv')",
   "id": "f409b326a15f9232",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:48:11.394210Z",
     "start_time": "2025-11-02T13:48:11.386292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_numeric_dtypes(df: pd.DataFrame, try_float16: bool = False, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to the smallest possible dtype without changing values.\n",
    "    - Integers: downcast to smallest signed/unsigned integer.\n",
    "    - Floats: downcast to float32 (and optionally float16 if lossless within tolerance).\n",
    "    Returns a new DataFrame (original unchanged).\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    start_mem = result.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    num_cols = [c for c in result.columns if pd.api.types.is_numeric_dtype(result[c])]\n",
    "    for c in num_cols:\n",
    "        col = result[c]\n",
    "\n",
    "        # Skip all-NaN\n",
    "        if col.notnull().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            # Integer (no NaNs)\n",
    "            if col.min() >= 0:\n",
    "                result[c] = pd.to_numeric(col, downcast=\"unsigned\")\n",
    "            else:\n",
    "                result[c] = pd.to_numeric(col, downcast=\"integer\")\n",
    "\n",
    "        elif pd.api.types.is_float_dtype(col):\n",
    "            # First, try float32\n",
    "            col32 = col.astype(np.float32)\n",
    "            if np.allclose(col.values, col32.values, equal_nan=True):\n",
    "                result[c] = col32\n",
    "                # Optionally try float16 (more aggressive)\n",
    "                if try_float16:\n",
    "                    col16 = col.astype(np.float16)\n",
    "                    if np.allclose(col.values, col16.astype(np.float32).values, rtol=1e-03, atol=1e-06, equal_nan=True):\n",
    "                        result[c] = col16\n",
    "            # else keep original float64\n",
    "\n",
    "        # If it's a nullable integer (Int64/Int32), try to preserve nulls with the smallest nullable int\n",
    "        elif pd.api.types.is_dtype_equal(col.dtype, \"Int64\") or str(col.dtype).startswith(\"Int\"):\n",
    "            if col.min() >= 0:\n",
    "                tmp = pd.to_numeric(col.astype(\"float64\"), downcast=\"unsigned\")\n",
    "            else:\n",
    "                tmp = pd.to_numeric(col.astype(\"float64\"), downcast=\"integer\")\n",
    "            # Cast back to nullable integer if still integer-like\n",
    "            if pd.api.types.is_integer_dtype(tmp):\n",
    "                result[c] = pd.Series(tmp, index=col.index).astype(pd.ArrowDtype(tmp.dtype.name) if hasattr(pd, \"ArrowDtype\") else tmp.dtype)\n",
    "\n",
    "    end_mem = result.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory: {start_mem:.2f} MB â†’ {end_mem:.2f} MB ({(start_mem-end_mem):.2f} MB saved, {(1 - end_mem/max(start_mem,1e-9))*100:.1f}% reduction)\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- Example ---\n",
    "# df_optimized = optimize_numeric_dtypes(df, try_float16=False, verbose=True)\n"
   ],
   "id": "1981f38243bd087e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:48:44.658760Z",
     "start_time": "2025-11-02T13:48:11.411914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df_small = optimize_numeric_dtypes(train_df.copy(), True, True)\n",
    "test_df_small = optimize_numeric_dtypes(test_df.copy(), True, True)\n",
    "\n"
   ],
   "id": "bb6f7cccec9383cb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170: RuntimeWarning: overflow encountered in cast\n",
      "  return arr.astype(dtype, copy=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 7706.79 MB â†’ 6101.18 MB (1605.61 MB saved, 20.8% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170: RuntimeWarning: overflow encountered in cast\n",
      "  return arr.astype(dtype, copy=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 3830.07 MB â†’ 3032.72 MB (797.35 MB saved, 20.8% reduction)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "1d30bbd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:48:44.681582Z",
     "start_time": "2025-11-02T13:48:44.676848Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:48:44.719616Z",
     "start_time": "2025-11-02T13:48:44.710449Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "162787adc5e6f26d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:50:52.811158Z",
     "start_time": "2025-11-02T13:48:44.748358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build train snapshots (fit scaler)\n",
    "train_snaps, train_ip2idx, scaler_edge, edge_cols_used = build_snapshots(\n",
    "    train_df_small, scaler_edge=None, fit_scaler=True, bin_seconds=300, device=device, include_per_bin_feats=False\n",
    ")\n",
    "\n",
    "# Build test snapshots (reuse scaler; separate ip2idx for strict inductive)\n",
    "test_snaps, test_ip2idx, _, _ = build_snapshots(\n",
    "    test_df_small, scaler_edge=scaler_edge, fit_scaler=False, bin_seconds=300, device=device, include_per_bin_feats=False\n",
    ")"
   ],
   "id": "174984bbd266c9b2",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "0b8b4853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T14:02:18.344473Z",
     "start_time": "2025-11-02T14:02:18.325671Z"
    }
   },
   "source": [
    "in_node = train_snaps[0].x.size(1)         # now includes centralities (+ optional per-bin feats)\n",
    "in_edge = train_snaps[0].edge_attr.size(1) # edge features + time enc\n",
    "model = GraphTimeEdgeClassifier(in_node=in_node, in_edge=in_edge,\n",
    "                                hidden=100, num_layers=2, dropout=0.2).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T14:12:47.292775Z",
     "start_time": "2025-11-02T14:02:29.839581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, os, copy, json\n",
    "ckpt_path = \"best_f1.pt\"   # change if you like\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_epoch = None\n",
    "best_state = None\n",
    "\n",
    "EPOCHS = 10\n",
    "batch_size = 4096*128\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_losses, tr_accs, tr_f1s, tr_fprs = [], [], [], []\n",
    "    for snap in train_snaps:\n",
    "        tr_loss, tr_acc, tr_f1, tr_fpr = run_epoch_neighbor(\n",
    "            model, snap, optimizer=opt, device=device,\n",
    "            num_neighbors=[25,10], batch_size=batch_size, shuffle=True\n",
    "        )\n",
    "        tr_losses.append(tr_loss if tr_loss is not None else 0.0)\n",
    "        tr_accs.append(tr_acc); tr_f1s.append(tr_f1); tr_fprs.append(tr_fpr)\n",
    "\n",
    "    # Eval on test with neighbor sampling (no optimizer)\n",
    "    te_accs, te_f1s, te_fprs = [], [], []\n",
    "    for snap in test_snaps:\n",
    "        _, te_acc, te_f1, te_fpr = run_epoch_neighbor(\n",
    "            model, snap, optimizer=None, device=device,\n",
    "            num_neighbors=[25,10], batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "        te_accs.append(te_acc); te_f1s.append(te_f1); te_fprs.append(te_fpr)\n",
    "\n",
    "    mean_tr_loss = float(np.mean(tr_losses))\n",
    "    mean_tr_acc  = float(np.mean(tr_accs))\n",
    "    mean_tr_f1   = float(np.mean(tr_f1s))\n",
    "    mean_tr_fpr  = float(np.mean(tr_fprs))\n",
    "\n",
    "    mean_te_acc  = float(np.mean(te_accs))\n",
    "    mean_te_f1   = float(np.mean(te_f1s))\n",
    "    mean_te_fpr  = float(np.mean(te_fprs))\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {mean_tr_loss:.4f} | train acc {mean_tr_acc:.4f} | \"\n",
    "          f\"train F1 {mean_tr_f1:.4f} | train FPR {mean_tr_fpr:.4f} | \"\n",
    "          f\"test acc {mean_te_acc:.4f} | test F1 {mean_te_f1:.4f} | test FPR {mean_te_fpr:.4f}\")\n",
    "\n",
    "    # ---- save best by TEST F1 ----\n",
    "    if mean_te_f1 > best_f1:\n",
    "        best_f1 = mean_te_f1\n",
    "        best_epoch = epoch\n",
    "        best_state = {\n",
    "            \"model_state\": copy.deepcopy(model.state_dict()),\n",
    "            \"epoch\": best_epoch,\n",
    "            \"best_test_f1\": best_f1,\n",
    "        }\n",
    "        torch.save(best_state, ckpt_path)\n",
    "        print(f\"âœ… Saved new best model @ epoch {epoch} (test F1={best_f1:.4f}) â†’ {ckpt_path}\")\n"
   ],
   "id": "d74889fd2959d224",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.0032 | train acc 0.9993 | train F1 0.9994 | train FPR 0.0006 | test acc 0.9993 | test F1 0.9990 | test FPR 0.0000\n",
      "âœ… Saved new best model @ epoch 1 (test F1=0.9990) â†’ best_f1.pt\n",
      "Epoch 02 | train loss 0.0013 | train acc 0.9993 | train F1 0.9995 | train FPR 0.0006 | test acc 0.9988 | test F1 0.9987 | test FPR 0.0005\n",
      "Epoch 03 | train loss 0.0010 | train acc 0.9999 | train F1 0.9998 | train FPR 0.0000 | test acc 0.9988 | test F1 0.9987 | test FPR 0.0006\n",
      "Epoch 04 | train loss 0.0003 | train acc 0.9999 | train F1 0.9999 | train FPR 0.0000 | test acc 0.9988 | test F1 0.9987 | test FPR 0.0005\n",
      "Epoch 05 | train loss 0.0002 | train acc 0.9999 | train F1 0.9999 | train FPR 0.0000 | test acc 0.9988 | test F1 0.9987 | test FPR 0.0006\n",
      "Epoch 06 | train loss 0.0001 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 0.9988 | test F1 0.9987 | test FPR 0.0006\n",
      "Epoch 07 | train loss 0.0006 | train acc 1.0000 | train F1 0.9999 | train FPR 0.0000 | test acc 0.9988 | test F1 0.9987 | test FPR 0.0005\n",
      "Epoch 08 | train loss 0.0001 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 0.9989 | test F1 0.9988 | test FPR 0.0005\n",
      "Epoch 09 | train loss 0.0002 | train acc 1.0000 | train F1 0.9999 | train FPR 0.0000 | test acc 0.9989 | test F1 0.9988 | test FPR 0.0005\n",
      "Epoch 10 | train loss 0.0000 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 0.9989 | test F1 0.9988 | test FPR 0.0005\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "4aef8224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T14:01:14.455576Z",
     "start_time": "2025-11-02T14:01:14.436849Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_with_confusion(model, snapshots, device='cpu'):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in snapshots:\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            labels = data.y.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred 0', 'Pred 1'],\n",
    "                yticklabels=['True 0', 'True 1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ðŸ“Š Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return cm, report"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluation",
   "id": "851f239e77f6bbfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:08.337843Z",
     "start_time": "2025-11-02T13:29:48.756193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt_path = \"best_f1.pt\"  # same path you saved to\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state[\"model_state\"])\n",
    "model.to(device).eval()   # ensure eval mode + on the right device\n",
    "\n",
    "cm, report = evaluate_with_confusion(model, test_snaps, device=device)\n"
   ],
   "id": "a539b26256be6ca3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\AppData\\Local\\Temp\\ipykernel_8980\\3316944521.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9858    1.0000    0.9928  12679742\n",
      "           1     0.0000    0.0000    0.0000    183030\n",
      "\n",
      "    accuracy                         0.9858  12862772\n",
      "   macro avg     0.4929    0.5000    0.4964  12862772\n",
      "weighted avg     0.9717    0.9858    0.9787  12862772\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Error analysis, trying to see which IP adresses failed",
   "id": "b11f3e801818e380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T14:13:12.663724Z",
     "start_time": "2025-11-02T14:12:55.027898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ckpt_path = \"best_f1.pt\"  # same path you saved to\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state[\"model_state\"])\n",
    "model.to(device).eval()   # ensure eval mode + on the right device\n",
    "\n",
    "cm, report = evaluate_with_confusion(model, test_snaps, device=device)\n"
   ],
   "id": "86947f102405c6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\AppData\\Local\\Temp\\ipykernel_25252\\3316944521.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(ckpt_path, map_location=device)\n",
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9858    1.0000    0.9928  12679742\n",
      "           1     0.0000    0.0000    0.0000    183030\n",
      "\n",
      "    accuracy                         0.9858  12862772\n",
      "   macro avg     0.4929    0.5000    0.4964  12862772\n",
      "weighted avg     0.9717    0.9858    0.9787  12862772\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:16.101577Z",
     "start_time": "2025-11-02T13:30:16.088630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collect_edge_predictions(model, snapshots, ip_idx_to_ip=None, device='cpu',\n",
    "                             keep_edge_cols=False):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with one row per edge prediction:\n",
    "    columns: ['bin','src_idx','dst_idx','src_ip','dst_ip','y_true','y_pred','p1','margin','loss_ce']\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in snapshots:\n",
    "            d = data.to(device)\n",
    "            logits = model(d)                         # [E,2]\n",
    "            probs  = F.softmax(logits, dim=1)[:,1]   # p(y=1)\n",
    "            y_true = d.y\n",
    "            y_pred = (probs >= 0.5).long()\n",
    "            per_ce = ce(logits, y_true)              # per-edge CE loss\n",
    "\n",
    "            # Map edge indices back to nodes\n",
    "            src, dst = d.edge_index\n",
    "            src = src.cpu().numpy()\n",
    "            dst = dst.cpu().numpy()\n",
    "\n",
    "            # Optional: map to IP strings if provided\n",
    "            if ip_idx_to_ip is not None:\n",
    "                src_ip = [ip_idx_to_ip.get(int(i), str(int(i))) for i in src]\n",
    "                dst_ip = [ip_idx_to_ip.get(int(i), str(int(i))) for i in dst]\n",
    "            else:\n",
    "                src_ip = [int(i) for i in src]\n",
    "                dst_ip = [int(i) for i in dst]\n",
    "\n",
    "            # Prediction stats\n",
    "            probs_np  = probs.cpu().numpy()\n",
    "            logits_np = logits.cpu().numpy()\n",
    "            margin_np = (logits_np[:,1] - logits_np[:,0])  # logit margin\n",
    "            y_true_np = y_true.cpu().numpy()\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            loss_np   = per_ce.cpu().numpy()\n",
    "\n",
    "            for i in range(len(y_true_np)):\n",
    "                rows.append({\n",
    "                    'bin': getattr(data, '_bin', -1),\n",
    "                    'src_idx': int(src[i]),\n",
    "                    'dst_idx': int(dst[i]),\n",
    "                    'src_ip': src_ip[i],\n",
    "                    'dst_ip': dst_ip[i],\n",
    "                    'y_true': int(y_true_np[i]),\n",
    "                    'y_pred': int(y_pred_np[i]),\n",
    "                    'p1': float(probs_np[i]),\n",
    "                    'margin': float(margin_np[i]),\n",
    "                    'loss_ce': float(loss_np[i]),\n",
    "                })\n",
    "    df_pred = pd.DataFrame(rows).sort_values(['bin','loss_ce'], ascending=[True, False])\n",
    "    return df_pred\n"
   ],
   "id": "8888a7f9276beff8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:18.377009Z",
     "start_time": "2025-11-02T13:30:18.367866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bin_metrics(df_pred):\n",
    "    out = []\n",
    "    for b, g in df_pred.groupby('bin'):\n",
    "        y, yhat = g['y_true'].values, g['y_pred'].values\n",
    "        p1 = g['p1'].values\n",
    "        tp = ((y==1)&(yhat==1)).sum()\n",
    "        fp = ((y==0)&(yhat==1)).sum()\n",
    "        tn = ((y==0)&(yhat==0)).sum()\n",
    "        fn = ((y==1)&(yhat==0)).sum()\n",
    "        acc = (tp+tn)/max(1,len(g))\n",
    "        prec = tp/max(1,tp+fp)\n",
    "        rec  = tp/max(1,tp+fn)\n",
    "        f1   = 2*prec*rec/max(1e-9,prec+rec)\n",
    "        try:\n",
    "            auc = roc_auc_score(y, p1) if (y.min()!=y.max()) else np.nan\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        out.append({'bin': b, 'n': len(g), 'acc':acc, 'precision':prec,\n",
    "                    'recall':rec, 'f1':f1, 'auc':auc, 'fp':fp, 'fn':fn})\n",
    "    return pd.DataFrame(out).sort_values('bin')\n"
   ],
   "id": "83223fbac837ec6e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:20.466139Z",
     "start_time": "2025-11-02T13:30:20.459919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def top_false_negatives(df_pred, k=20):\n",
    "    return df_pred[(df_pred.y_true==1)&(df_pred.y_pred==0)].sort_values('margin').head(k)\n",
    "\n",
    "def top_false_positives(df_pred, k=20):\n",
    "    return df_pred[(df_pred.y_true==0)&(df_pred.y_pred==1)].sort_values('p1', ascending=False).head(k)\n"
   ],
   "id": "412538aceee5d4d7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:21.531936Z",
     "start_time": "2025-11-02T13:30:21.523390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def offender_table(df_pred, by='src_ip', k=30):\n",
    "    g = df_pred.groupby(by).apply(\n",
    "        lambda t: pd.Series({\n",
    "            'n': len(t),\n",
    "            'fn': int(((t.y_true==1)&(t.y_pred==0)).sum()),\n",
    "            'fp': int(((t.y_true==0)&(t.y_pred==1)).sum()),\n",
    "            'err_rate': float((t.y_true!=t.y_pred).mean())\n",
    "        })\n",
    "    ).reset_index()\n",
    "    g['fn_share'] = g['fn']/g['n'].clip(lower=1)\n",
    "    g['fp_share'] = g['fp']/g['n'].clip(lower=1)\n",
    "    return g.sort_values(['err_rate','n'], ascending=[False,False]).head(k)\n"
   ],
   "id": "f0d0167ac5a69a03",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:23.107458Z",
     "start_time": "2025-11-02T13:30:23.101249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def tune_threshold(df_pred, fn_cost=5.0, fp_cost=1.0):\n",
    "    \"\"\"\n",
    "    Choose threshold that minimizes cost = fn_cost*FN + fp_cost*FP.\n",
    "    \"\"\"\n",
    "    y = df_pred['y_true'].values\n",
    "    p = df_pred['p1'].values\n",
    "    # Sweep thresholds via precision-recall curve\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    # Convert to FPs/FNs given counts\n",
    "    P = (y==1).sum()\n",
    "    N = (y==0).sum()\n",
    "    # recall = TP/P -> TP = recall*P\n",
    "    # precision = TP/(TP+FP) -> FP = TP*(1/precision - 1)\n",
    "    TP = rec * P\n",
    "    FP = TP * (1/np.clip(prec,1e-9,None) - 1)\n",
    "    FN = P - TP\n",
    "    cost = fn_cost*FN + fp_cost*FP\n",
    "    idx = np.nanargmin(cost)\n",
    "    best_thr = thr[idx-1] if idx>0 and idx-1 < len(thr) else 0.5\n",
    "    stats = {'best_thr': float(best_thr), 'cost': float(cost[idx]),\n",
    "             'recall': float(rec[idx]), 'precision': float(prec[idx])}\n",
    "    return best_thr, stats\n"
   ],
   "id": "8bf0698450c1759c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:30:25.604560Z",
     "start_time": "2025-11-02T13:30:25.599267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def error_by_feature_quantiles(df_joined, feature, q=10):\n",
    "    \"\"\"\n",
    "    df_joined: df_pred merged with raw edge features for the same edge rows\n",
    "               must contain columns: feature, y_true, y_pred\n",
    "    \"\"\"\n",
    "    cuts = pd.qcut(df_joined[feature], q=q, duplicates='drop')\n",
    "    g = df_joined.groupby(cuts).apply(lambda t: pd.Series({\n",
    "        'n': len(t),\n",
    "        'err_rate': float((t.y_true!=t.y_pred).mean()),\n",
    "        'fn_rate': float(((t.y_true==1)&(t.y_pred==0)).mean()),\n",
    "        'fp_rate': float(((t.y_true==0)&(t.y_pred==1)).mean()),\n",
    "        'avg_p1': float(t['p1'].mean())\n",
    "    })).reset_index().rename(columns={feature:'bin_'+feature})\n",
    "    return g.sort_values('err_rate', ascending=False)\n"
   ],
   "id": "7236dd08b931049a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:34:11.700204Z",
     "start_time": "2025-11-02T13:30:27.027836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After each test epoch:\n",
    "idx2ip_train = {v:k for k,v in train_ip2idx.items()}\n",
    "idx2ip_test  = {v:k for k,v in test_ip2idx.items()}\n",
    "\n",
    "df_train_pred = collect_edge_predictions(model, train_snaps, idx2ip_train, device=device)\n",
    "df_test_pred  = collect_edge_predictions(model, test_snaps,  idx2ip_test,  device=device)\n",
    "\n",
    "print(\"\\n=== Per-bin (TEST) ===\")\n",
    "print(bin_metrics(df_test_pred).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Top FN (TEST) ===\")\n",
    "print(top_false_negatives(df_test_pred, k=15)[['bin','src_ip','dst_ip','p1','margin','loss_ce']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Top FP (TEST) ===\")\n",
    "print(top_false_positives(df_test_pred, k=15)[['bin','src_ip','dst_ip','p1','margin','loss_ce']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Offenders by src_ip (TEST) ===\")\n",
    "print(offender_table(df_test_pred, by='src_ip', k=20).to_string(index=False))\n",
    "\n",
    "# Threshold tuning globally (you can also do this per bin)\n",
    "best_thr, stats = tune_threshold(df_train_pred, fn_cost=5.0, fp_cost=1.0)\n",
    "print(f\"\\n[Threshold] best {best_thr:.3f} | recall={stats['recall']:.3f} precision={stats['precision']:.3f}\")\n"
   ],
   "id": "ed075f25a1a45939",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-bin (TEST) ===\n",
      "    bin     n      acc  precision  recall  f1      auc  fp   fn\n",
      "5066355 69015 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066356 65243 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066357 72318 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066358 56812 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066359 53291 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066360 56213 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066361 58177 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066362 63135 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066363 58675 0.999949        0.0     0.0 0.0      NaN   3    0\n",
      "5066364 58612 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066365 49598 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066366 57352 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066367 65861 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066368 57852 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066369 58785 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066370 55021 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066371 64641 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066372 64172 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066373 69184 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066374 67995 0.999691        0.0     0.0 0.0 0.005772   0   21\n",
      "5066375 66513 0.999805        0.0     0.0 0.0 0.101907   0   13\n",
      "5066376 66322 0.999955        0.0     0.0 0.0 0.035042   1    2\n",
      "5066377 53409 0.999888        0.0     0.0 0.0 0.046349   0    6\n",
      "5066378 62795 0.962051        0.0     0.0 0.0 0.415076   0 2383\n",
      "5066379 74734 0.994554        0.0     0.0 0.0 0.127536   0  407\n",
      "5066380 69749 0.936558        0.0     0.0 0.0 0.663591   0 4425\n",
      "5066381 71933 0.939249        0.0     0.0 0.0 0.322817   0 4370\n",
      "5066382 66460 0.999940        0.0     0.0 0.0 0.049537   0    4\n",
      "5066383 59637 0.995858        0.0     0.0 0.0 0.187713   0  247\n",
      "5066384 70299 0.964807        0.0     0.0 0.0 0.343150   0 2474\n",
      "5066385 74347 0.940858        0.0     0.0 0.0 0.498364   0 4397\n",
      "5066386 63467 0.931902        0.0     0.0 0.0 0.384477   0 4322\n",
      "5066387 65147 0.999156        0.0     0.0 0.0 0.037616   0   55\n",
      "5066388 64345 0.999953        0.0     0.0 0.0 0.001849   0    3\n",
      "5066389 56480 0.999965        0.0     0.0 0.0 0.003028   0    2\n",
      "5066390 62821 0.999952        0.0     0.0 0.0 0.001799   0    3\n",
      "5066391 67925 0.999971        0.0     0.0 0.0 0.003033   0    2\n",
      "5066392 61051 0.999951        0.0     0.0 0.0 0.012711   0    3\n",
      "5066393 62160 0.999968        0.0     0.0 0.0 0.013498   0    2\n",
      "5066394 59117 0.999949        0.0     0.0 0.0 0.011655   0    3\n",
      "5066395 55075 0.999964        0.0     0.0 0.0 0.006337   0    2\n",
      "5066396 62424 0.999952        0.0     0.0 0.0 0.003284   0    3\n",
      "5066397 65511 0.999969        0.0     0.0 0.0 0.002030   0    2\n",
      "5066398 63579 0.999953        0.0     0.0 0.0 0.001777   0    3\n",
      "5066399 56896 0.999965        0.0     0.0 0.0 0.001389   0    2\n",
      "5066400 47231 0.999936        0.0     0.0 0.0 0.001313   0    3\n",
      "5066401 45404 0.999956        0.0     0.0 0.0 0.001762   0    2\n",
      "5066402 44906 0.999933        0.0     0.0 0.0 0.002227   0    3\n",
      "5066403 48855 0.999959        0.0     0.0 0.0 0.002211   0    2\n",
      "5066404 45583 0.999934        0.0     0.0 0.0 0.004234   0    3\n",
      "5066405 50953 0.999961        0.0     0.0 0.0 0.001786   0    2\n",
      "5066406 45298 0.999934        0.0     0.0 0.0 0.003267   0    3\n",
      "5066407 41828 0.999952        0.0     0.0 0.0 0.001243   0    2\n",
      "5066408 42123 0.999929        0.0     0.0 0.0 0.000831   0    3\n",
      "5066409 45990 0.999957        0.0     0.0 0.0 0.000500   0    2\n",
      "5066410 49206 0.999939        0.0     0.0 0.0 0.000610   0    3\n",
      "5066411 51626 0.999961        0.0     0.0 0.0 0.000910   0    2\n",
      "5066412 46505 0.999935        0.0     0.0 0.0 0.001312   0    3\n",
      "5066413 50803 0.999961        0.0     0.0 0.0 0.002087   0    2\n",
      "5066414 52092 0.999942        0.0     0.0 0.0 0.000538   0    3\n",
      "5066415 57720 0.999965        0.0     0.0 0.0 0.001403   0    2\n",
      "5066416 57345 0.999948        0.0     0.0 0.0 0.001918   0    3\n",
      "5066417 52606 0.999962        0.0     0.0 0.0 0.003213   0    2\n",
      "5066418 53247 0.999944        0.0     0.0 0.0 0.001315   0    3\n",
      "5066419 67378 0.999970        0.0     0.0 0.0 0.001202   0    2\n",
      "5066420 71132 0.999972        0.0     0.0 0.0 0.001462   0    2\n",
      "5066421 68293 0.999956        0.0     0.0 0.0 0.001581   0    3\n",
      "5066422 62686 0.999968        0.0     0.0 0.0 0.003956   0    2\n",
      "5066423 64167 0.999953        0.0     0.0 0.0 0.002930   0    3\n",
      "5066424 63929 0.999969        0.0     0.0 0.0 0.001377   0    2\n",
      "5066425 67982 0.999956        0.0     0.0 0.0 0.000736   0    3\n",
      "5066426 62001 0.999968        0.0     0.0 0.0 0.003484   0    2\n",
      "5066427 60538 0.999950        0.0     0.0 0.0 0.000754   0    3\n",
      "5066428 63018 0.999952        0.0     0.0 0.0 0.002338   0    3\n",
      "5066429 61321 0.995825        0.0     0.0 0.0 0.216596   0  256\n",
      "5066430 69491 0.965708        0.0     0.0 0.0 0.309452   0 2383\n",
      "5066431 62131 0.960970        0.0     0.0 0.0 0.713049   0 2425\n",
      "5066432 66043 0.904168        0.0     0.0 0.0 0.460910   0 6329\n",
      "5066433 55087 0.997767        0.0     0.0 0.0 0.135615   0  123\n",
      "5066434 56601 0.999965        0.0     0.0 0.0 0.001855   0    2\n",
      "5066435 63060 0.999952        0.0     0.0 0.0 0.001380   0    3\n",
      "5066436 66509 0.999970        0.0     0.0 0.0 0.000481   0    2\n",
      "5066437 60528 0.999950        0.0     0.0 0.0 0.001437   0    3\n",
      "5066438 66446 0.999970        0.0     0.0 0.0 0.002227   0    2\n",
      "5066439 57524 0.999948        0.0     0.0 0.0 0.003773   0    3\n",
      "5066440 54235 0.997824        0.0     0.0 0.0 0.189820   0  118\n",
      "5066441 66262 0.963146        0.0     0.0 0.0 0.426161   0 2442\n",
      "5066442 58039 0.995245        0.0     0.0 0.0 0.117300   0  276\n",
      "5066443 60652 0.962887        0.0     0.0 0.0 0.321270   0 2251\n",
      "5066444 58372 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066445 57813 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066446 55976 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066447 64652 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066448 59572 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066449 59346 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066450 55886 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066451 55354 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066452 57034 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066453 61125 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066454 59518 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066455 60850 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066456 60355 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066457 52084 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066458 57783 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066459 62276 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066460 52393 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066461 42873 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066462 38408 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066463 39121 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066464 35039 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066465 41803 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066466  8160 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066467  4354 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066468  3561 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066469  3263 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066470  2426 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066471  2130 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066472  1992 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066473  1398 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066474   972 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066475   894 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066476   617 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066477   727 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066478   757 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066479   705 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066480   477 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066481   503 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066482   501 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066483   392 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066484   376 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066485   301 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066486   220 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066487   141 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066488    89 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066489    99 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066490   107 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066491    81 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066492     3 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066649 57463 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066650 72524 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066651 59421 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066652 61142 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066653 55246 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066654 61505 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066655 62177 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066656 55905 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066657 59577 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066658 55502 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066659 53619 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066660 68945 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066661 77428 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066662 69907 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066663 66037 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066664 59930 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066665 59345 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066666 51242 0.999102        0.0     0.0 0.0 0.181935   0   46\n",
      "5066667 61503 0.996179        0.0     0.0 0.0 0.123879   0  235\n",
      "5066668 58682 0.992417        0.0     0.0 0.0 0.122237   0  445\n",
      "5066669 55748 0.992000        0.0     0.0 0.0 0.071073   0  446\n",
      "5066670 55454 0.987864        0.0     0.0 0.0 0.108152   0  673\n",
      "5066671 61994 0.982821        0.0     0.0 0.0 0.120193   0 1065\n",
      "5066672 58726 0.976092        0.0     0.0 0.0 0.149618   0 1404\n",
      "5066673 75609 0.938499        0.0     0.0 0.0 0.215088   0 4650\n",
      "5066674 66282 0.964410        0.0     0.0 0.0 0.208593   0 2359\n",
      "5066675 57708 0.974198        0.0     0.0 0.0 0.339574   0 1489\n",
      "5066676 69136 0.978463        0.0     0.0 0.0 0.253323   0 1489\n",
      "5066677 66178 0.928723        0.0     0.0 0.0 0.402231   0 4717\n",
      "5066678 66838 0.916455        0.0     0.0 0.0 0.165360   1 5583\n",
      "5066679 82895 0.925267        0.0     0.0 0.0 0.171665   0 6195\n",
      "5066680 71155 0.912922        0.0     0.0 0.0 0.123311   0 6196\n",
      "5066681 74714 0.918262        0.0     0.0 0.0 0.168589   0 6107\n",
      "5066682 61819 0.900678        0.0     0.0 0.0 0.355240   0 6140\n",
      "5066683 57028 0.967192        0.0     0.0 0.0 0.276676   0 1871\n",
      "5066684 61598 0.978246        0.0     0.0 0.0 0.274859   0 1340\n",
      "5066685 66361 0.979807        0.0     0.0 0.0 0.281575   0 1340\n",
      "5066686 63585 0.978957        0.0     0.0 0.0 0.195365   0 1338\n",
      "5066687 62360 0.978528        0.0     0.0 0.0 0.226115   0 1339\n",
      "5066688 56190 0.976152        0.0     0.0 0.0 0.301536   0 1340\n",
      "5066689 47144 0.971576        0.0     0.0 0.0 0.162335   0 1340\n",
      "5066690 45592 0.973943        0.0     0.0 0.0 0.214329   0 1188\n",
      "5066691 54008 0.975189        0.0     0.0 0.0 0.138990   0 1340\n",
      "5066692 51127 0.973830        0.0     0.0 0.0 0.128321   0 1338\n",
      "5066693 56166 0.976196        0.0     0.0 0.0 0.109318   0 1337\n",
      "5066694 48592 0.972423        0.0     0.0 0.0 0.109062   0 1340\n",
      "5066695 42225 0.968242        0.0     0.0 0.0 0.114240   0 1341\n",
      "5066696 44814 0.970121        0.0     0.0 0.0 0.113228   0 1339\n",
      "5066697 50172 0.973272        0.0     0.0 0.0 0.087733   0 1341\n",
      "5066698 53579 0.975046        0.0     0.0 0.0 0.096193   0 1337\n",
      "5066699 52703 0.974593        0.0     0.0 0.0 0.102963   0 1339\n",
      "5066700 48593 0.972403        0.0     0.0 0.0 0.116993   0 1341\n",
      "5066701 47307 0.971696        0.0     0.0 0.0 0.142062   0 1339\n",
      "5066702 52862 0.974651        0.0     0.0 0.0 0.281186   0 1340\n",
      "5066703 55178 0.975769        0.0     0.0 0.0 0.252388   0 1337\n",
      "5066704 53850 0.975097        0.0     0.0 0.0 0.194446   0 1341\n",
      "5066705 68365 0.980414        0.0     0.0 0.0 0.126807   0 1339\n",
      "5066706 61604 0.978248        0.0     0.0 0.0 0.126750   0 1340\n",
      "5066707 71069 0.981173        0.0     0.0 0.0 0.124172   0 1338\n",
      "5066708 65883 0.979661        0.0     0.0 0.0 0.139099   0 1340\n",
      "5066709 65152 0.979448        0.0     0.0 0.0 0.122528   0 1339\n",
      "5066710 64506 0.979227        0.0     0.0 0.0 0.137114   0 1340\n",
      "5066711 64649 0.979288        0.0     0.0 0.0 0.163048   0 1339\n",
      "5066712 62071 0.978396        0.0     0.0 0.0 0.120713   0 1341\n",
      "5066713 63184 0.978824        0.0     0.0 0.0 0.110143   0 1338\n",
      "5066714 59803 0.977627        0.0     0.0 0.0 0.254067   0 1338\n",
      "5066715 60988 0.978028        0.0     0.0 0.0 0.312486   0 1340\n",
      "5066716 60721 0.961496        0.0     0.0 0.0 0.096457   0 2338\n",
      "5066717 62902 0.919764        0.0     0.0 0.0 0.144503   0 5047\n",
      "5066718 60919 0.977987        0.0     0.0 0.0 0.155498   0 1341\n",
      "5066719 58796 0.977226        0.0     0.0 0.0 0.188320   0 1339\n",
      "5066720 61657 0.978299        0.0     0.0 0.0 0.160666   0 1338\n",
      "5066721 59456 0.977513        0.0     0.0 0.0 0.201956   0 1337\n",
      "5066722 59121 0.977318        0.0     0.0 0.0 0.148179   0 1341\n",
      "5066723 64112 0.979083        0.0     0.0 0.0 0.137694   0 1341\n",
      "5066724 66224 0.979796        0.0     0.0 0.0 0.094557   0 1338\n",
      "5066725 68060 0.980370        0.0     0.0 0.0 0.103480   0 1336\n",
      "5066726 59080 0.977319        0.0     0.0 0.0 0.157849   0 1340\n",
      "5066727 57934 0.976887        0.0     0.0 0.0 0.213988   0 1339\n",
      "5066728 58624 0.977177        0.0     0.0 0.0 0.199097   0 1338\n",
      "5066729 65230 0.979457        0.0     0.0 0.0 0.123782   0 1340\n",
      "5066730 65517 0.928553        0.0     0.0 0.0 0.086505   0 4681\n",
      "5066731 68281 0.917971        0.0     0.0 0.0 0.064051   0 5601\n",
      "5066732 67617 0.917210        0.0     0.0 0.0 0.047323   0 5598\n",
      "5066733 67613 0.917294        0.0     0.0 0.0 0.076284   0 5592\n",
      "5066734 60843 0.927009        0.0     0.0 0.0 0.083733   0 4441\n",
      "5066735 62059 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066736 61527 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066737 61384 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066738 56882 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066739 57049 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066740 56218 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066741 63521 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066742 62958 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066743 58210 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066744 62885 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066745 58478 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066746 53536 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066747 65260 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066748 56672 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066749 42877 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066750 44736 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066751 43066 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066752 37316 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066753 47878 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066754  8361 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066755  3677 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066756  2788 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066757  3189 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066758  2777 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066759  2128 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066760  1439 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066761   893 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066762  1057 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066763   763 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066764   724 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066765   873 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066766   635 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066767   646 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066768   652 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066769   695 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066770   657 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066771   599 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066772   316 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066773   326 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066774   290 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066775   388 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066776   362 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066777   298 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066778   274 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066779   204 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066780   189 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066781   110 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066782   106 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066783    88 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066784   117 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066785   129 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066786   106 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066787   183 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066788   141 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066789   115 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066790   112 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "5066791    81 1.000000        0.0     0.0 0.0      NaN   0    0\n",
      "\n",
      "=== Top FN (TEST) ===\n",
      "    bin       src_ip        dst_ip           p1     margin   loss_ce\n",
      "5066374 172.31.69.13   162.125.3.1 2.947867e-14 -31.155109 31.155109\n",
      "5066427 172.31.69.13  13.58.225.34 3.270556e-14 -31.051231 31.051231\n",
      "5066376 172.31.69.13  13.58.225.34 4.051744e-14 -30.837044 30.837044\n",
      "5066423 172.31.69.13  13.58.225.34 4.315537e-14 -30.773970 30.773970\n",
      "5066423 172.31.69.13  13.58.225.34 4.315726e-14 -30.773926 30.773926\n",
      "5066423 172.31.69.13  13.58.225.34 4.315759e-14 -30.773918 30.773918\n",
      "5066428 172.31.69.13  13.58.225.34 4.483564e-14 -30.735773 30.735773\n",
      "5066374 172.31.69.13 104.16.100.29 4.628138e-14 -30.704037 30.704037\n",
      "5066428 172.31.69.13  13.58.225.34 4.642381e-14 -30.700964 30.700964\n",
      "5066374 172.31.69.13 104.16.100.29 4.875764e-14 -30.651915 30.651915\n",
      "5066425 172.31.69.13  13.58.225.34 5.102053e-14 -30.606548 30.606548\n",
      "5066425 172.31.69.13  13.58.225.34 5.102082e-14 -30.606543 30.606543\n",
      "5066425 172.31.69.13  13.58.225.34 5.102092e-14 -30.606541 30.606541\n",
      "5066432 172.31.69.13  172.31.69.20 5.417244e-14 -30.546604 30.546604\n",
      "5066432 172.31.69.13  172.31.69.20 5.417244e-14 -30.546604 30.546604\n",
      "\n",
      "=== Top FP (TEST) ===\n",
      "    bin         src_ip        dst_ip       p1   margin  loss_ce\n",
      "5066376 222.186.50.156  172.31.69.28 0.745443 1.074453 1.368230\n",
      "5066678 185.222.211.98  172.31.69.28 0.651371 0.625070 1.053746\n",
      "5066363   172.31.69.28  91.189.95.83 0.623968 0.506425 0.978081\n",
      "5066363   172.31.69.28 91.189.88.162 0.588208 0.356562 0.887237\n",
      "5066363   172.31.69.28 52.15.102.108 0.581115 0.327352 0.870159\n",
      "\n",
      "=== Offenders by src_ip (TEST) ===\n",
      "        src_ip        n      fn  fp  err_rate  fn_share  fp_share\n",
      "  172.31.69.13  66890.0 39847.0 0.0  0.595709  0.595709  0.000000\n",
      "  172.31.69.30  28568.0 15886.0 0.0  0.556077  0.556077  0.000000\n",
      "   172.31.69.8  28435.0 15189.0 0.0  0.534166  0.534166  0.000000\n",
      "  172.31.69.17  28708.0 15317.0 0.0  0.533545  0.533545  0.000000\n",
      "  172.31.69.10  29656.0 15811.0 0.0  0.533147  0.533147  0.000000\n",
      "  172.31.69.26  28758.0 14870.0 0.0  0.517074  0.517074  0.000000\n",
      "  172.31.69.14  29565.0 15260.0 0.0  0.516151  0.516151  0.000000\n",
      "  172.31.69.23  30025.0 15237.0 0.0  0.507477  0.507477  0.000000\n",
      "   172.31.69.6  31150.0 15705.0 0.0  0.504173  0.504173  0.000000\n",
      "  172.31.69.29  31734.0 14917.0 0.0  0.470064  0.470064  0.000000\n",
      "  172.31.69.12  18131.0  4991.0 0.0  0.275274  0.275274  0.000000\n",
      "185.222.211.98     80.0     0.0 1.0  0.012500  0.000000  0.012500\n",
      "222.186.50.156    168.0     0.0 1.0  0.005952  0.000000  0.005952\n",
      "  172.31.69.28   1281.0     0.0 3.0  0.002342  0.000000  0.002342\n",
      "   5.101.40.43 231634.0     0.0 0.0  0.000000  0.000000  0.000000\n",
      "  5.101.40.105 220127.0     0.0 0.0  0.000000  0.000000  0.000000\n",
      "  24.234.124.9 100743.0     0.0 0.0  0.000000  0.000000  0.000000\n",
      "  212.92.116.6  80592.0     0.0 0.0  0.000000  0.000000  0.000000\n",
      " 212.92.107.25  72522.0     0.0 0.0  0.000000  0.000000  0.000000\n",
      "  196.52.84.25  71801.0     0.0 0.0  0.000000  0.000000  0.000000\n",
      "\n",
      "[Threshold] best 0.082 | recall=1.000 precision=1.000\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we will see which IPs repeat in both training and test set.",
   "id": "9d93fe9523a26c6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:34:53.689865Z",
     "start_time": "2025-11-02T13:34:39.719680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SRC = 'Src IP'\n",
    "DST = 'Dst IP'\n",
    "\n",
    "def ip_overlap_report(train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                      src=SRC, dst=DST) -> pd.DataFrame:\n",
    "    # counts per IP in each split (count both src and dst appearances)\n",
    "    tr_counts = pd.concat([train_df[src].astype(str), train_df[dst].astype(str)]).value_counts()\n",
    "    te_counts = pd.concat([test_df[src].astype(str),  test_df[dst].astype(str)]).value_counts()\n",
    "\n",
    "    # align on the union of IPs\n",
    "    all_ips = tr_counts.index.union(te_counts.index)\n",
    "    df = pd.DataFrame({\n",
    "        'train_count': tr_counts.reindex(all_ips, fill_value=0),\n",
    "        'test_count' : te_counts.reindex(all_ips, fill_value=0),\n",
    "    })\n",
    "    df['in_train'] = df['train_count'] > 0\n",
    "    df['in_test']  = df['test_count']  > 0\n",
    "    df['status']   = np.where(df['in_train'] & df['in_test'], 'both',\n",
    "                       np.where(df['in_train'], 'train_only', 'test_only'))\n",
    "    return df.sort_values(['status','train_count','test_count'], ascending=[True, False, False])\n",
    "\n",
    "def edge_overlap_counts(train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                        src=SRC, dst=DST):\n",
    "    # sets of directed edges (src,dst) as strings\n",
    "    tr_pairs = set(zip(train_df[src].astype(str), train_df[dst].astype(str)))\n",
    "    te_pairs = set(zip(test_df[src].astype(str),  test_df[dst].astype(str)))\n",
    "\n",
    "    both      = tr_pairs & te_pairs\n",
    "    train_only= tr_pairs - te_pairs\n",
    "    test_only = te_pairs - tr_pairs\n",
    "\n",
    "    print(f'Edges â€” train unique: {len(tr_pairs):,}, test unique: {len(te_pairs):,}')\n",
    "    print(f'Edges in both: {len(both):,} | only in train: {len(train_only):,} | only in test: {len(test_only):,}')\n",
    "    return tr_pairs, te_pairs, both, train_only, test_only\n",
    "\n",
    "# ---- run it ----\n",
    "ip_df = ip_overlap_report(train_df, test_df)\n",
    "\n",
    "print(f'Unique IPs â€” train: {ip_df.in_train.sum():,}, test: {ip_df.in_test.sum():,}')\n",
    "print(f'IPs in both: {(ip_df.status==\"both\").sum():,} | only in train: {(ip_df.status==\"train_only\").sum():,} | only in test: {(ip_df.status==\"test_only\").sum():,}')\n",
    "\n",
    "# peek at each group (top 10 by total count)\n",
    "display(ip_df.query('status==\"both\"').assign(total=ip_df.train_count+ip_df.test_count).sort_values('total', ascending=False).head(10))\n",
    "display(ip_df.query('status==\"train_only\"').sort_values('train_count', ascending=False).head(10))\n",
    "display(ip_df.query('status==\"test_only\"').sort_values('test_count', ascending=False).head(10))\n",
    "\n",
    "# save full lists if useful\n",
    "ip_df.query('status==\"both\"').to_csv('ips_in_both.csv')\n",
    "ip_df.query('status==\"train_only\"').to_csv('ips_train_only.csv')\n",
    "ip_df.query('status==\"test_only\"').to_csv('ips_test_only.csv')\n",
    "\n",
    "# (optional) do the same analysis for edges\n",
    "_ = edge_overlap_counts(train_df, test_df)\n"
   ],
   "id": "2ef652b85a9e6273",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique IPs â€” train: 94,905, test: 56,495\n",
      "IPs in both: 21,737 | only in train: 73,168 | only in test: 34,758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 train_count  test_count  in_train  in_test status     total\n",
       "172.31.0.2          10080402     4519097      True     True   both  14599499\n",
       "172.31.69.25         2491900        2641      True     True   both   2494541\n",
       "169.254.169.254      1139718      529475      True     True   both   1669193\n",
       "5.101.40.105          243174      220146      True     True   both    463320\n",
       "5.101.40.43           178045      231647      True     True   both    409692\n",
       "212.92.116.6          157039       80606      True     True   both    237645\n",
       "72.21.91.29           158517       77350      True     True   both    235867\n",
       "172.31.64.103         131615       66031      True     True   both    197646\n",
       "172.31.64.83          122313       74454      True     True   both    196767\n",
       "172.31.65.56           83593       57038      True     True   both    140631"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "      <th>in_train</th>\n",
       "      <th>in_test</th>\n",
       "      <th>status</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172.31.0.2</th>\n",
       "      <td>10080402</td>\n",
       "      <td>4519097</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>14599499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.25</th>\n",
       "      <td>2491900</td>\n",
       "      <td>2641</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>2494541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169.254.169.254</th>\n",
       "      <td>1139718</td>\n",
       "      <td>529475</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>1669193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.101.40.105</th>\n",
       "      <td>243174</td>\n",
       "      <td>220146</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>463320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.101.40.43</th>\n",
       "      <td>178045</td>\n",
       "      <td>231647</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>409692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212.92.116.6</th>\n",
       "      <td>157039</td>\n",
       "      <td>80606</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>237645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.21.91.29</th>\n",
       "      <td>158517</td>\n",
       "      <td>77350</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>235867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.64.103</th>\n",
       "      <td>131615</td>\n",
       "      <td>66031</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>197646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.64.83</th>\n",
       "      <td>122313</td>\n",
       "      <td>74454</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>196767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.65.56</th>\n",
       "      <td>83593</td>\n",
       "      <td>57038</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "      <td>140631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                 train_count  test_count  in_train  in_test      status\n",
       "18.219.193.20        1803246           0      True    False  train_only\n",
       "18.221.219.4          193354           0      True    False  train_only\n",
       "175.195.219.31        118464           0      True    False  train_only\n",
       "13.59.126.31          105550           0      True    False  train_only\n",
       "212.92.114.68         104823           0      True    False  train_only\n",
       "152.101.118.11         95057           0      True    False  train_only\n",
       "13.58.98.64            94237           0      True    False  train_only\n",
       "145.239.183.169        76780           0      True    False  train_only\n",
       "77.243.191.23          75507           0      True    False  train_only\n",
       "69.16.250.107          74297           0      True    False  train_only"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "      <th>in_train</th>\n",
       "      <th>in_test</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18.219.193.20</th>\n",
       "      <td>1803246</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.221.219.4</th>\n",
       "      <td>193354</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175.195.219.31</th>\n",
       "      <td>118464</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.59.126.31</th>\n",
       "      <td>105550</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212.92.114.68</th>\n",
       "      <td>104823</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152.101.118.11</th>\n",
       "      <td>95057</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.58.98.64</th>\n",
       "      <td>94237</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145.239.183.169</th>\n",
       "      <td>76780</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.243.191.23</th>\n",
       "      <td>75507</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.16.250.107</th>\n",
       "      <td>74297</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>train_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "                 train_count  test_count  in_train  in_test     status\n",
       "18.219.211.138             0      143183     False     True  test_only\n",
       "212.92.107.25              0       72528     False     True  test_only\n",
       "196.52.84.25               0       71803     False     True  test_only\n",
       "87.230.72.8                0       60719     False     True  test_only\n",
       "219.135.224.15             0       52834     False     True  test_only\n",
       "165.132.169.106            0       46722     False     True  test_only\n",
       "221.126.225.109            0       37988     False     True  test_only\n",
       "212.92.124.11              0       37831     False     True  test_only\n",
       "189.236.198.113            0       31938     False     True  test_only\n",
       "80.241.219.91              0       31789     False     True  test_only"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_count</th>\n",
       "      <th>test_count</th>\n",
       "      <th>in_train</th>\n",
       "      <th>in_test</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18.219.211.138</th>\n",
       "      <td>0</td>\n",
       "      <td>143183</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212.92.107.25</th>\n",
       "      <td>0</td>\n",
       "      <td>72528</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196.52.84.25</th>\n",
       "      <td>0</td>\n",
       "      <td>71803</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87.230.72.8</th>\n",
       "      <td>0</td>\n",
       "      <td>60719</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219.135.224.15</th>\n",
       "      <td>0</td>\n",
       "      <td>52834</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165.132.169.106</th>\n",
       "      <td>0</td>\n",
       "      <td>46722</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221.126.225.109</th>\n",
       "      <td>0</td>\n",
       "      <td>37988</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212.92.124.11</th>\n",
       "      <td>0</td>\n",
       "      <td>37831</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189.236.198.113</th>\n",
       "      <td>0</td>\n",
       "      <td>31938</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.241.219.91</th>\n",
       "      <td>0</td>\n",
       "      <td>31789</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>test_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges â€” train unique: 1,164,890, test unique: 683,335\n",
      "Edges in both: 211,652 | only in train: 953,238 | only in test: 471,683\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:34:53.809738Z",
     "start_time": "2025-11-02T13:34:53.699402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SRC, DST = 'Src IP', 'Dst IP'\n",
    "\n",
    "def where_ips_live(train_df, test_df, ips=None, src=SRC, dst=DST):\n",
    "    tr_src = train_df[src].astype(str)\n",
    "    tr_dst = train_df[dst].astype(str)\n",
    "    te_src = test_df[src].astype(str)\n",
    "    te_dst = test_df[dst].astype(str)\n",
    "\n",
    "    # counts per role\n",
    "    tr_src_ct = tr_src.value_counts()\n",
    "    tr_dst_ct = tr_dst.value_counts()\n",
    "    te_src_ct = te_src.value_counts()\n",
    "    te_dst_ct = te_dst.value_counts()\n",
    "\n",
    "    # which IPs to check (all or a subset)\n",
    "    if ips is None:\n",
    "        ips = sorted(set(tr_src_ct.index) | set(tr_dst_ct.index) |\n",
    "                     set(te_src_ct.index) | set(te_dst_ct.index))\n",
    "    idx = pd.Index(map(str, ips), name='ip')\n",
    "\n",
    "    out = pd.DataFrame(index=idx)\n",
    "    out['train_src'] = out.index.map(tr_src_ct).fillna(0).astype(int)\n",
    "    out['train_dst'] = out.index.map(tr_dst_ct).fillna(0).astype(int)\n",
    "    out['test_src']  = out.index.map(te_src_ct).fillna(0).astype(int)\n",
    "    out['test_dst']  = out.index.map(te_dst_ct).fillna(0).astype(int)\n",
    "\n",
    "    out['train_total'] = out['train_src'] + out['train_dst']\n",
    "    out['test_total']  = out['test_src']  + out['test_dst']\n",
    "    out['in_train'] = out['train_total'] > 0\n",
    "    out['in_test']  = out['test_total']  > 0\n",
    "    out['status'] = np.select(\n",
    "        [out.in_train & out.in_test, out.in_train, out.in_test],\n",
    "        ['both','train_only','test_only'],\n",
    "        default='neither'\n",
    "    )\n",
    "    # nice ordering\n",
    "    return out.sort_values(['status','test_total','train_total'], ascending=[True, False, False])\n"
   ],
   "id": "86cec8eb22979be4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T13:34:59.521703Z",
     "start_time": "2025-11-02T13:34:55.356035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "offender_ips = [\n",
    "    '172.31.69.13','172.31.69.30','172.31.69.8','172.31.69.17','172.31.69.10',\n",
    "    '172.31.69.26','172.31.69.14','172.31.69.23','172.31.69.6','172.31.69.29', '172.31.69.12'\n",
    "]\n",
    "display(where_ips_live(train_df, test_df, offender_ips))\n"
   ],
   "id": "6e0870ab79b29ec6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              train_src  train_dst  test_src  test_dst  train_total  \\\n",
       "ip                                                                    \n",
       "172.31.69.13      52208       8466     66890      9720        60674   \n",
       "172.31.69.29      46216      12401     31734      9642        58617   \n",
       "172.31.69.6       49473      15378     31150      9827        64851   \n",
       "172.31.69.23      52717      12109     30025      8307        64826   \n",
       "172.31.69.10      51224      10631     29656      8492        61855   \n",
       "172.31.69.30      47013      16957     28568      6955        63970   \n",
       "172.31.69.26      49934       9974     28758      6528        59908   \n",
       "172.31.69.14      44215      18067     29565      5419        62282   \n",
       "172.31.69.17      45756      17078     28708      5978        62834   \n",
       "172.31.69.8       48829      13930     28435      6241        62759   \n",
       "172.31.69.12      44940      11698     18131      7083        56638   \n",
       "\n",
       "              test_total  in_train  in_test status  \n",
       "ip                                                  \n",
       "172.31.69.13       76610      True     True   both  \n",
       "172.31.69.29       41376      True     True   both  \n",
       "172.31.69.6        40977      True     True   both  \n",
       "172.31.69.23       38332      True     True   both  \n",
       "172.31.69.10       38148      True     True   both  \n",
       "172.31.69.30       35523      True     True   both  \n",
       "172.31.69.26       35286      True     True   both  \n",
       "172.31.69.14       34984      True     True   both  \n",
       "172.31.69.17       34686      True     True   both  \n",
       "172.31.69.8        34676      True     True   both  \n",
       "172.31.69.12       25214      True     True   both  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_src</th>\n",
       "      <th>train_dst</th>\n",
       "      <th>test_src</th>\n",
       "      <th>test_dst</th>\n",
       "      <th>train_total</th>\n",
       "      <th>test_total</th>\n",
       "      <th>in_train</th>\n",
       "      <th>in_test</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ip</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172.31.69.13</th>\n",
       "      <td>52208</td>\n",
       "      <td>8466</td>\n",
       "      <td>66890</td>\n",
       "      <td>9720</td>\n",
       "      <td>60674</td>\n",
       "      <td>76610</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.29</th>\n",
       "      <td>46216</td>\n",
       "      <td>12401</td>\n",
       "      <td>31734</td>\n",
       "      <td>9642</td>\n",
       "      <td>58617</td>\n",
       "      <td>41376</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.6</th>\n",
       "      <td>49473</td>\n",
       "      <td>15378</td>\n",
       "      <td>31150</td>\n",
       "      <td>9827</td>\n",
       "      <td>64851</td>\n",
       "      <td>40977</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.23</th>\n",
       "      <td>52717</td>\n",
       "      <td>12109</td>\n",
       "      <td>30025</td>\n",
       "      <td>8307</td>\n",
       "      <td>64826</td>\n",
       "      <td>38332</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.10</th>\n",
       "      <td>51224</td>\n",
       "      <td>10631</td>\n",
       "      <td>29656</td>\n",
       "      <td>8492</td>\n",
       "      <td>61855</td>\n",
       "      <td>38148</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.30</th>\n",
       "      <td>47013</td>\n",
       "      <td>16957</td>\n",
       "      <td>28568</td>\n",
       "      <td>6955</td>\n",
       "      <td>63970</td>\n",
       "      <td>35523</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.26</th>\n",
       "      <td>49934</td>\n",
       "      <td>9974</td>\n",
       "      <td>28758</td>\n",
       "      <td>6528</td>\n",
       "      <td>59908</td>\n",
       "      <td>35286</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.14</th>\n",
       "      <td>44215</td>\n",
       "      <td>18067</td>\n",
       "      <td>29565</td>\n",
       "      <td>5419</td>\n",
       "      <td>62282</td>\n",
       "      <td>34984</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.17</th>\n",
       "      <td>45756</td>\n",
       "      <td>17078</td>\n",
       "      <td>28708</td>\n",
       "      <td>5978</td>\n",
       "      <td>62834</td>\n",
       "      <td>34686</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.8</th>\n",
       "      <td>48829</td>\n",
       "      <td>13930</td>\n",
       "      <td>28435</td>\n",
       "      <td>6241</td>\n",
       "      <td>62759</td>\n",
       "      <td>34676</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172.31.69.12</th>\n",
       "      <td>44940</td>\n",
       "      <td>11698</td>\n",
       "      <td>18131</td>\n",
       "      <td>7083</td>\n",
       "      <td>56638</td>\n",
       "      <td>25214</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
