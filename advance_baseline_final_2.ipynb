{
 "cells": [
  {
   "cell_type": "code",
   "id": "ff190f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:55:57.866112Z",
     "start_time": "2025-11-06T06:55:57.851056Z"
    }
   },
   "source": [
    "import os, math, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pandas import read_csv\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader, NeighborLoader\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import networkx as nx"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "e2d61ef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:55:58.865242Z",
     "start_time": "2025-11-06T06:55:58.860985Z"
    }
   },
   "source": [
    "\n",
    "EDGE_COLS = [\n",
    "    'Bwd Packet Length Min', 'Protocol_6', 'Bwd Packets/s', 'FWD Init Win Bytes',\n",
    "    'Packet Length Std', 'FIN Flag Count', 'SrcPortRange_registered',\n",
    "    'Packet Length Min', 'Fwd Seg Size Min', 'DstPortRange_well_known',\n",
    "    'Bwd IAT Total', 'SYN Flag Count', 'Bwd Packet Length Std'\n",
    "]\n",
    "ID_COLS = ['Src IP','Dst IP','Timestamp']\n",
    "LABEL_COL = 'target'"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3662ee25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:55:59.762302Z",
     "start_time": "2025-11-06T06:55:59.754864Z"
    }
   },
   "source": [
    "# def time_posenc(t, periods=(60, 300, 3600)):\n",
    "#     # t: numpy array of epoch seconds\n",
    "#     feats = []\n",
    "#     for P in periods:\n",
    "#         w = 2*math.pi/P\n",
    "#         feats.append(np.sin(w*t))\n",
    "#         feats.append(np.cos(w*t))\n",
    "#     return np.stack(feats, axis=1)  # [N, 2*len(periods)]\n",
    "\n",
    "def bin_time(df, bin_seconds=300):\n",
    "    # Expect df['Timestamp'] as datetime or string; convert to seconds\n",
    "    ts = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True).astype('int64') // 10**9\n",
    "    df = df.copy()\n",
    "    df['_epoch'] = ts\n",
    "    df['_bin'] = (ts // bin_seconds).astype(int)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "0913f22b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:56:15.496078Z",
     "start_time": "2025-11-06T06:56:15.479936Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "def compute_node_centralities_fast2(\n",
    "    df: pd.DataFrame,\n",
    "    ip2idx: dict,\n",
    "    src_col: str = \"Src IP\",\n",
    "    dst_col: str = \"Dst IP\",\n",
    "    use_betweenness: bool = False,     # set True if you have NetworKit\n",
    "    betw_samples: int = 32,            # NetworKit ApproxBetweenness samples\n",
    "    pagerank_alpha: float = 0.85,\n",
    "    pagerank_iters: int = 40,\n",
    "    pagerank_tol: float = 1e-6,\n",
    "    closeness_if_small: int = 20_000,  # only compute closeness if N <= this\n",
    "    ktruss_if_small: int = 10_000      # only compute k-truss if N <= this\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns C: [N_nodes, 5] -> [degree, betweenness, closeness, pagerank, ktruss_level], z-scored per column.\n",
    "    Fast path fills unavailable metrics with 0 (safe after z-scoring).\n",
    "    \"\"\"\n",
    "    N = len(ip2idx)\n",
    "    if N == 0:\n",
    "        return np.zeros((0,5), dtype=float)\n",
    "\n",
    "    # ------- Build sparse adjacency (directed) fast -------\n",
    "    # Map to indices without Python loops\n",
    "    src_idx = df[src_col].astype(str).map(ip2idx).to_numpy()\n",
    "    dst_idx = df[dst_col].astype(str).map(ip2idx).to_numpy()\n",
    "    mask = np.isfinite(src_idx) & np.isfinite(dst_idx)\n",
    "    src_idx = src_idx[mask].astype(np.int64, copy=False)\n",
    "    dst_idx = dst_idx[mask].astype(np.int64, copy=False)\n",
    "\n",
    "    # Remove self-loops once (optional)\n",
    "    non_self = src_idx != dst_idx\n",
    "    src_idx, dst_idx = src_idx[non_self], dst_idx[non_self]\n",
    "\n",
    "    data = np.ones_like(src_idx, dtype=np.float64)\n",
    "    A = sparse.coo_matrix((data, (src_idx, dst_idx)), shape=(N, N)).tocsr()\n",
    "\n",
    "    # ------- Degree (total degree for directed) -------\n",
    "    outdeg = A.getnnz(axis=1)          # rows\n",
    "    indeg  = A.getnnz(axis=0)          # cols\n",
    "    degree = (outdeg + indeg).astype(np.float64)\n",
    "\n",
    "    # ------- PageRank (power iteration on sparse) -------\n",
    "    # Row-normalize A^T equivalent: P^T @ pr\n",
    "    outdeg_safe = np.maximum(outdeg, 1)\n",
    "    Dinv = sparse.diags(1.0 / outdeg_safe)\n",
    "    P = Dinv @ A                       # row-stochastic (on rows)\n",
    "    pr = np.full(N, 1.0 / N, dtype=np.float64)\n",
    "    teleport = (1.0 - pagerank_alpha) / N\n",
    "    for _ in range(pagerank_iters):\n",
    "        pr_new = pagerank_alpha * (P.T @ pr) + teleport\n",
    "        if np.linalg.norm(pr_new - pr, 1) < pagerank_tol:\n",
    "            pr = pr_new\n",
    "            break\n",
    "        pr = pr_new\n",
    "\n",
    "    # ------- Betweenness (optional, NetworKit) -------\n",
    "    betw = np.zeros(N, dtype=np.float64)\n",
    "    if use_betweenness:\n",
    "        try:\n",
    "            import networkit as nk\n",
    "            # Build NetworKit graph\n",
    "            Gnk = nk.Graph(n=N, weighted=False, directed=True)\n",
    "            # Add edges (NetworKit expects int indices)\n",
    "            # Faster add: iterate CSR rows\n",
    "            rows, cols = A.nonzero()\n",
    "            for u, v in zip(rows.tolist(), cols.tolist()):\n",
    "                if u != v:\n",
    "                    Gnk.addEdge(u, v)\n",
    "            c = nk.centrality.ApproxBetweenness(Gnk, nSamples=int(betw_samples), normalized=True)\n",
    "            c.run()\n",
    "            betw = np.array(c.scores(), dtype=np.float64)\n",
    "        except Exception:\n",
    "            # If NetworKit not available, keep zeros (safe after z-score)\n",
    "            pass\n",
    "\n",
    "    # ------- Closeness (tiny graphs only, else zeros) -------\n",
    "    clos = np.zeros(N, dtype=np.float64)\n",
    "    if N <= closeness_if_small:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            H = nx.from_scipy_sparse_array(A, create_using=nx.DiGraph)\n",
    "            # Use NX fast approximation? (still Python; okay for small N)\n",
    "            clos_dict = nx.closeness_centrality(H)  # directed-version\n",
    "            # Map dict to array by index order\n",
    "            # NetworkX labels are 0..N-1 when built from scipy sparse\n",
    "            clos = np.array([clos_dict.get(i, 0.0) for i in range(N)], dtype=np.float64)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ------- k-truss (tiny graphs only, else zeros) -------\n",
    "    ktr = np.zeros(N, dtype=np.float64)\n",
    "    if N <= ktruss_if_small:\n",
    "        try:\n",
    "            import networkx as nx\n",
    "            Hu = nx.from_scipy_sparse_array((A + A.T).sign(), create_using=nx.Graph)\n",
    "            ktr_level = np.zeros(N, dtype=np.int32)\n",
    "            for k in range(3, 7):  # modest bound; raise carefully\n",
    "                try:\n",
    "                    Tk = nx.k_truss(Hu, k)\n",
    "                except nx.NetworkXError:\n",
    "                    break\n",
    "                nodes = list(Tk.nodes())\n",
    "                if not nodes:\n",
    "                    continue\n",
    "                ktr_level[np.array(nodes, dtype=np.int64)] = np.maximum(ktr_level[np.array(nodes, dtype=np.int64)], k)\n",
    "            ktr = ktr_level.astype(np.float64)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ------- Pack in the right slot order: [degree, betweenness, closeness, pagerank, ktruss] -------\n",
    "    C = np.column_stack([degree, betw, clos, pr, ktr])\n",
    "\n",
    "    # ------- z-score per column (robust to zeros) -------\n",
    "    mu = C.mean(axis=0, keepdims=True)\n",
    "    sd = C.std(axis=0, keepdims=True) + 1e-8\n",
    "    C = (C - mu) / sd\n",
    "    return C\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "27190bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:56:19.837668Z",
     "start_time": "2025-11-06T06:56:19.825676Z"
    }
   },
   "source": [
    "# Fast unique IP mapping (string-safe)\n",
    "def make_ip_index(df, src_col='Src IP', dst_col='Dst IP'):\n",
    "    # Combine columns as one numpy array without concat copies\n",
    "    src = df[src_col].astype(str).to_numpy(copy=False)\n",
    "    dst = df[dst_col].astype(str).to_numpy(copy=False)\n",
    "    all_ips = np.concatenate((src, dst))\n",
    "\n",
    "    # Use pandas categorical (internally fast hash-based unique)\n",
    "    cat = pd.Categorical(all_ips)\n",
    "    ip2idx = dict(zip(cat.categories, range(len(cat.categories))))\n",
    "    n_nodes = len(ip2idx)\n",
    "    return ip2idx, n_nodes\n",
    "\n",
    "def build_snapshots(df, scaler_edge=None, fit_scaler=False, bin_seconds=300, device='cpu', include_per_bin_feats=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      snapshots: list[Data] in time order\n",
    "      ip2idx: dict mapping IP -> node index (per full dataset, stable across train/test)\n",
    "      scaler_edge: fitted StandardScaler for edge features\n",
    "      edge_cols_kept: list of columns used (existing + non-NA + time enc + centralities names)\n",
    "    \"\"\"\n",
    "    # Keep only available columns\n",
    "    edge_cols = [c for c in EDGE_COLS if c in df.columns]\n",
    "    cols_needed = ID_COLS + edge_cols + [LABEL_COL]\n",
    "    cols_needed = [c for c in cols_needed if c in df.columns]\n",
    "    df = df[cols_needed].dropna(subset=['Src IP','Dst IP'])\n",
    "    df = bin_time(df, bin_seconds=bin_seconds)\n",
    "\n",
    "    edge_cols = [c for c in EDGE_COLS if c in df.columns]\n",
    "    for c in edge_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df[edge_cols] = df[edge_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # Edge feature scaler\n",
    "    if scaler_edge is None:\n",
    "        scaler_edge = StandardScaler()\n",
    "        fit_scaler = True\n",
    "    if fit_scaler and len(edge_cols) > 0:\n",
    "        scaler_edge.fit(df[edge_cols].astype(float).values)\n",
    "\n",
    "    ip2idx, n_nodes = make_ip_index(df)\n",
    "\n",
    "    # --- NEW: centralities over the full graph ---\n",
    "    C = compute_node_centralities_fast2( df, ip2idx,use_betweenness=False,     betw_samples=32,       pagerank_iters=40)\n",
    "\n",
    "    snapshots = []\n",
    "    prev_activity = defaultdict(int)  # lag-1 activity per node\n",
    "\n",
    "    # iterate bins\n",
    "    for b, g in df.sort_values('_bin').groupby('_bin'):\n",
    "        # Map nodes\n",
    "        src = g['Src IP'].map(ip2idx).astype(int).values\n",
    "        dst = g['Dst IP'].map(ip2idx).astype(int).values\n",
    "        edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
    "\n",
    "        # Edge attributes = scaled flow features + time encoding\n",
    "        if len(edge_cols) > 0:\n",
    "            eX = scaler_edge.transform(g[edge_cols].astype(float).values)\n",
    "        else:\n",
    "            eX = np.empty((len(g), 0), dtype=float)\n",
    "        # tfe = time_posenc(g['_epoch'].values)  # [E, 2*len(periods)]\n",
    "        # edge_attr_np = np.hstack([eX, tfe])\n",
    "        edge_attr_np = np.hstack([eX])\n",
    "        edge_attr = torch.tensor(edge_attr_np, dtype=torch.float)\n",
    "        edge_attr = torch.nan_to_num(edge_attr, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "        # Labels (edge-level)\n",
    "        y = torch.tensor(g[LABEL_COL].astype(int).values, dtype=torch.long)\n",
    "\n",
    "        # Node features:\n",
    "        #   (paper) centralities C; (yours) optional per-bin degrees + prev_activity\n",
    "        if include_per_bin_feats:\n",
    "            out_deg = np.bincount(src, minlength=n_nodes)\n",
    "            in_deg  = np.bincount(dst, minlength=n_nodes)\n",
    "            deg     = (out_deg + in_deg).reshape(-1,1)\n",
    "            node_feat = np.hstack([\n",
    "                in_deg.reshape(-1,1),\n",
    "                out_deg.reshape(-1,1),\n",
    "                deg,\n",
    "                np.array([prev_activity[i] for i in range(n_nodes)]).reshape(-1,1)\n",
    "            ])\n",
    "            node_feat = np.log1p(node_feat)\n",
    "            x_np = np.hstack([node_feat, C])  # [N, 4 + 5]\n",
    "        else:\n",
    "            x_np = C  # strict paper-style init\n",
    "\n",
    "        x = torch.tensor(x_np, dtype=torch.float)\n",
    "        x = torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y\n",
    "        )\n",
    "        data._bin = int(b)\n",
    "        snapshots.append(data)\n",
    "\n",
    "        # Update prev_activity for next bin (count edges touched by node this bin)\n",
    "        touched = np.bincount(np.concatenate([src, dst]), minlength=n_nodes)\n",
    "        for i, c in enumerate(touched):\n",
    "            prev_activity[i] = int(c)\n",
    "\n",
    "    # Column names returned (only for info)\n",
    "    # time_cols = [f'time_{i}' for i in range(tfe.shape[1])]\n",
    "    # cent_cols = ['cent_degree','cent_betweenness','cent_closeness','cent_pagerank','cent_ktruss']\n",
    "    # edge_cols_used = edge_cols + time_cols + cent_cols\n",
    "\n",
    "    # time_cols = [f'time_{i}' for i in range(tfe.shape[1])]\n",
    "    cent_cols = ['cent_degree','cent_betweenness','cent_closeness','cent_pagerank','cent_ktruss']\n",
    "    edge_cols_used = edge_cols + cent_cols\n",
    "    return snapshots, ip2idx, scaler_edge, edge_cols_used"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "6472af76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:56:22.882579Z",
     "start_time": "2025-11-06T06:56:22.867777Z"
    }
   },
   "source": [
    "class EdgeGraphSAGEConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Edge-aware GraphSAGE (E-GraphSAGE-like):\n",
    "      m_ij = gate([x_i, x_j, e_ij]) * Ï†( Wj x_j + Wi x_i + We e_ij )\n",
    "      h_i' = Norm( mean_j m_ij + Wself x_i )     (residual)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, edge_in: int, out_channels: int, aggr: str = \"mean\", dropout: float = 0.0):\n",
    "        super().__init__(aggr=aggr, node_dim=0)\n",
    "        self.lin_src  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.lin_dst  = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.lin_edge = nn.Linear(edge_in,    out_channels, bias=False)\n",
    "        self.lin_self = nn.Linear(in_channels, out_channels, bias=True)\n",
    "\n",
    "        # Small gate that decides how much of each message passes through\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(in_channels + in_channels + edge_in, out_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)  # -> [N, Fout]\n",
    "        out = out + self.lin_self(x)  # residual\n",
    "        out = self.norm(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        msg_raw = self.lin_src(x_j) + self.lin_dst(x_i) + self.lin_edge(edge_attr)\n",
    "        g = torch.sigmoid(self.gate(torch.cat([x_i, x_j, edge_attr], dim=-1)))\n",
    "        msg = F.relu(msg_raw) * g\n",
    "        return self.dropout(msg)\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor) -> torch.Tensor:\n",
    "        return aggr_out"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ba02d20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:30:31.068066Z",
     "start_time": "2025-11-06T07:30:31.053042Z"
    }
   },
   "source": [
    "class GraphTimeEdgeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    E-GraphSAGE-style backbone that uses edge attributes inside message passing\n",
    "    AND in the final edge-level head.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_node: int, in_edge: int, hidden: int = 32, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        layers = []\n",
    "        dims = [in_node] + [hidden] * num_layers\n",
    "        for i in range(num_layers):\n",
    "            layers.append(EdgeGraphSAGEConv(dims[i], in_edge, dims[i+1], aggr=\"mean\", dropout=dropout))\n",
    "        self.convs = nn.ModuleList(layers)\n",
    "\n",
    "        # Edge MLP head: combine node embeddings and edge features\n",
    "        edge_head_in = (2 * hidden) + in_edge + hidden + hidden  # [h_s, h_d, e, |h_s-h_d|, h_s*h_d]\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(edge_head_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Data) -> torch.Tensor:\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        src, dst = edge_index\n",
    "        h_s, h_d = x[src], x[dst]\n",
    "        h_abs = torch.abs(h_s - h_d)\n",
    "        h_mul = h_s * h_d\n",
    "        z = torch.cat([h_s, h_d, edge_attr, h_abs, h_mul], dim=-1)\n",
    "\n",
    "        logits = self.edge_mlp(z)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "1a2a7676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:56:27.682970Z",
     "start_time": "2025-11-06T06:56:27.673250Z"
    }
   },
   "source": [
    "def run_epoch_neighbor(model, snapshot: Data, optimizer=None, device='cuda',\n",
    "                       num_neighbors=[25,10], batch_size=4096, shuffle=True):\n",
    "    is_train = optimizer is not None\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total_loss, total_correct, total_edges = 0.0, 0, 0\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    loader = NeighborLoader(snapshot, num_neighbors=num_neighbors, batch_size=batch_size, shuffle=shuffle)\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "\n",
    "        if is_train:\n",
    "            loss = ce(logits, batch.y)\n",
    "            if not torch.isfinite(loss):\n",
    "                # skip this minibatch if something went bad\n",
    "                continue\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item()) * batch.y.numel()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(dim=1)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_trues.append(batch.y.cpu().numpy())\n",
    "            total_correct += int((pred == batch.y).sum())\n",
    "            total_edges += int(batch.y.numel())\n",
    "\n",
    "    # Metrics\n",
    "    if all_trues:\n",
    "        y_true = np.concatenate(all_trues)\n",
    "        y_pred = np.concatenate(all_preds)\n",
    "        weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)\n",
    "        fpr = fp / max(1, (fp + tn))\n",
    "    else:\n",
    "        weighted_f1, fpr = float('nan'), float('nan')\n",
    "\n",
    "    avg_loss = (total_loss / max(1, total_edges)) if is_train else None\n",
    "    acc = total_correct / max(1, total_edges)\n",
    "    return avg_loss, acc, weighted_f1, fpr"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3f867f79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:57:11.388408Z",
     "start_time": "2025-11-06T06:56:30.208826Z"
    }
   },
   "source": "train_df = pd.read_csv('train.csv')\n",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:57:21.627821Z",
     "start_time": "2025-11-06T06:57:11.400933Z"
    }
   },
   "cell_type": "code",
   "source": "test_df = pd.read_csv('test.csv')",
   "id": "f409b326a15f9232",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:57:21.703769Z",
     "start_time": "2025-11-06T06:57:21.686689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_numeric_dtypes(df: pd.DataFrame, try_float16: bool = False, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to the smallest possible dtype without changing values.\n",
    "    - Integers: downcast to smallest signed/unsigned integer.\n",
    "    - Floats: downcast to float32 (and optionally float16 if lossless within tolerance).\n",
    "    Returns a new DataFrame (original unchanged).\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    start_mem = result.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    num_cols = [c for c in result.columns if pd.api.types.is_numeric_dtype(result[c])]\n",
    "    for c in num_cols:\n",
    "        col = result[c]\n",
    "\n",
    "        # Skip all-NaN\n",
    "        if col.notnull().sum() == 0:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_integer_dtype(col):\n",
    "            # Integer (no NaNs)\n",
    "            if col.min() >= 0:\n",
    "                result[c] = pd.to_numeric(col, downcast=\"unsigned\")\n",
    "            else:\n",
    "                result[c] = pd.to_numeric(col, downcast=\"integer\")\n",
    "\n",
    "        elif pd.api.types.is_float_dtype(col):\n",
    "            # First, try float32\n",
    "            col32 = col.astype(np.float32)\n",
    "            if np.allclose(col.values, col32.values, equal_nan=True):\n",
    "                result[c] = col32\n",
    "                # Optionally try float16 (more aggressive)\n",
    "                if try_float16:\n",
    "                    col16 = col.astype(np.float16)\n",
    "                    if np.allclose(col.values, col16.astype(np.float32).values, rtol=1e-03, atol=1e-06, equal_nan=True):\n",
    "                        result[c] = col16\n",
    "            # else keep original float64\n",
    "\n",
    "        # If it's a nullable integer (Int64/Int32), try to preserve nulls with the smallest nullable int\n",
    "        elif pd.api.types.is_dtype_equal(col.dtype, \"Int64\") or str(col.dtype).startswith(\"Int\"):\n",
    "            if col.min() >= 0:\n",
    "                tmp = pd.to_numeric(col.astype(\"float64\"), downcast=\"unsigned\")\n",
    "            else:\n",
    "                tmp = pd.to_numeric(col.astype(\"float64\"), downcast=\"integer\")\n",
    "            # Cast back to nullable integer if still integer-like\n",
    "            if pd.api.types.is_integer_dtype(tmp):\n",
    "                result[c] = pd.Series(tmp, index=col.index).astype(pd.ArrowDtype(tmp.dtype.name) if hasattr(pd, \"ArrowDtype\") else tmp.dtype)\n",
    "\n",
    "    end_mem = result.memory_usage(deep=True).sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f\"Memory: {start_mem:.2f} MB â†’ {end_mem:.2f} MB ({(start_mem-end_mem):.2f} MB saved, {(1 - end_mem/max(start_mem,1e-9))*100:.1f}% reduction)\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- Example ---\n",
    "# df_optimized = optimize_numeric_dtypes(df, try_float16=False, verbose=True)\n"
   ],
   "id": "1981f38243bd087e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:57:51.389140Z",
     "start_time": "2025-11-06T06:57:21.731322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df_small = optimize_numeric_dtypes(train_df.copy(), True, True)\n",
    "test_df_small = optimize_numeric_dtypes(test_df.copy(), True, True)\n",
    "\n"
   ],
   "id": "bb6f7cccec9383cb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170: RuntimeWarning: overflow encountered in cast\n",
      "  return arr.astype(dtype, copy=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 7706.79 MB â†’ 6101.18 MB (1605.61 MB saved, 20.8% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alice\\PycharmProjects\\co-simulation-code\\.venv1\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170: RuntimeWarning: overflow encountered in cast\n",
      "  return arr.astype(dtype, copy=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: 1926.65 MB â†’ 1525.25 MB (401.40 MB saved, 20.8% reduction)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "1d30bbd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:57:51.443127Z",
     "start_time": "2025-11-06T06:57:51.436466Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:57:51.492140Z",
     "start_time": "2025-11-06T06:57:51.479718Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "162787adc5e6f26d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T06:59:43.612704Z",
     "start_time": "2025-11-06T06:57:51.499187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build train snapshots (fit scaler)\n",
    "train_snaps, train_ip2idx, scaler_edge, edge_cols_used = build_snapshots(\n",
    "    train_df_small, scaler_edge=None, fit_scaler=True, bin_seconds=300, device=device, include_per_bin_feats=False\n",
    ")\n",
    "\n",
    "# Build test snapshots (reuse scaler; separate ip2idx for strict inductive)\n",
    "test_snaps, test_ip2idx, _, _ = build_snapshots(\n",
    "    test_df_small, scaler_edge=scaler_edge, fit_scaler=False, bin_seconds=300, device=device, include_per_bin_feats=False\n",
    ")"
   ],
   "id": "174984bbd266c9b2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "0b8b4853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:30:54.646994Z",
     "start_time": "2025-11-06T07:30:54.634971Z"
    }
   },
   "source": [
    "in_node = train_snaps[0].x.size(1)         # now includes centralities (+ optional per-bin feats)\n",
    "in_edge = train_snaps[0].edge_attr.size(1) # edge features + time enc\n",
    "model = GraphTimeEdgeClassifier(in_node=in_node, in_edge=in_edge,\n",
    "                                hidden=32, num_layers=2, dropout=0.2).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:52:37.989709Z",
     "start_time": "2025-11-06T07:30:56.804118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_losses, tr_accs, tr_f1s, tr_fprs = [], [], [], []\n",
    "    for snap in train_snaps:\n",
    "        tr_loss, tr_acc, tr_f1, tr_fpr = run_epoch_neighbor(model, snap, optimizer=opt, device=device,\n",
    "                                                            num_neighbors=[25,10], batch_size=4096, shuffle=True)\n",
    "        tr_losses.append(tr_loss if tr_loss is not None else 0.0)\n",
    "        tr_accs.append(tr_acc); tr_f1s.append(tr_f1); tr_fprs.append(tr_fpr)\n",
    "    # Eval on test with neighbor sampling (no optimizer)\n",
    "    te_accs, te_f1s, te_fprs = [], [], []\n",
    "    for snap in test_snaps:\n",
    "        _, te_acc, te_f1, te_fpr = run_epoch_neighbor(model, snap, optimizer=None, device=device,\n",
    "                                                        num_neighbors=[25,10], batch_size=4096, shuffle=False)\n",
    "        te_accs.append(te_acc); te_f1s.append(te_f1); te_fprs.append(te_fpr)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "            f\"train loss {np.mean(tr_losses):.4f} | train acc {np.mean(tr_accs):.4f} | \"\n",
    "            f\"train F1 {np.mean(tr_f1s):.4f} | train FPR {np.mean(tr_fprs):.4f} | \"\n",
    "            f\"test acc {np.mean(te_accs):.4f} | test F1 {np.mean(te_f1s):.4f} | test FPR {np.mean(te_fprs):.4f}\")"
   ],
   "id": "39f0c9afad94b839",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.0007 | train acc 0.9998 | train F1 0.9999 | train FPR 0.0002 | test acc 1.0000 | test F1 1.0000 | test FPR 0.0000\n",
      "Epoch 02 | train loss 0.0001 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 0.9999 | test F1 0.9999 | test FPR 0.0001\n",
      "Epoch 03 | train loss 0.0000 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 1.0000 | test F1 1.0000 | test FPR 0.0000\n",
      "Epoch 04 | train loss 0.0000 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 0.9999 | test F1 0.9999 | test FPR 0.0001\n",
      "Epoch 05 | train loss 0.0000 | train acc 1.0000 | train F1 1.0000 | train FPR 0.0000 | test acc 0.9999 | test F1 0.9999 | test FPR 0.0001\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "4aef8224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:52:44.814406Z",
     "start_time": "2025-11-06T07:52:44.794460Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_with_confusion(model, snapshots, device='cpu'):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in snapshots:\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            labels = data.y.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Pred 0', 'Pred 1'],\n",
    "                yticklabels=['True 0', 'True 1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ðŸ“Š Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return cm, report"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluation",
   "id": "851f239e77f6bbfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:52:55.823314Z",
     "start_time": "2025-11-06T07:52:47.075789Z"
    }
   },
   "cell_type": "code",
   "source": "cm, report = evaluate_with_confusion(model, test_snaps, device=device)\n",
   "id": "3b456dbcc790267e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9999    1.0000   5391219\n",
      "           1     0.9996    1.0000    0.9998   1084194\n",
      "\n",
      "    accuracy                         0.9999   6475413\n",
      "   macro avg     0.9998    1.0000    0.9999   6475413\n",
      "weighted avg     0.9999    0.9999    0.9999   6475413\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Error analysis, trying to see which IP adresses failed",
   "id": "b11f3e801818e380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:54:04.480295Z",
     "start_time": "2025-11-06T07:54:04.462421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collect_edge_predictions(model, snapshots, ip_idx_to_ip=None, device='cpu',\n",
    "                             keep_edge_cols=False):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with one row per edge prediction:\n",
    "    columns: ['bin','src_idx','dst_idx','src_ip','dst_ip','y_true','y_pred','p1','margin','loss_ce']\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    ce = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in snapshots:\n",
    "            d = data.to(device)\n",
    "            logits = model(d)                         # [E,2]\n",
    "            probs  = F.softmax(logits, dim=1)[:,1]   # p(y=1)\n",
    "            y_true = d.y\n",
    "            y_pred = (probs >= 0.5).long()\n",
    "            per_ce = ce(logits, y_true)              # per-edge CE loss\n",
    "\n",
    "            # Map edge indices back to nodes\n",
    "            src, dst = d.edge_index\n",
    "            src = src.cpu().numpy()\n",
    "            dst = dst.cpu().numpy()\n",
    "\n",
    "            # Optional: map to IP strings if provided\n",
    "            if ip_idx_to_ip is not None:\n",
    "                src_ip = [ip_idx_to_ip.get(int(i), str(int(i))) for i in src]\n",
    "                dst_ip = [ip_idx_to_ip.get(int(i), str(int(i))) for i in dst]\n",
    "            else:\n",
    "                src_ip = [int(i) for i in src]\n",
    "                dst_ip = [int(i) for i in dst]\n",
    "\n",
    "            # Prediction stats\n",
    "            probs_np  = probs.cpu().numpy()\n",
    "            logits_np = logits.cpu().numpy()\n",
    "            margin_np = (logits_np[:,1] - logits_np[:,0])  # logit margin\n",
    "            y_true_np = y_true.cpu().numpy()\n",
    "            y_pred_np = y_pred.cpu().numpy()\n",
    "            loss_np   = per_ce.cpu().numpy()\n",
    "\n",
    "            for i in range(len(y_true_np)):\n",
    "                rows.append({\n",
    "                    'bin': getattr(data, '_bin', -1),\n",
    "                    'src_idx': int(src[i]),\n",
    "                    'dst_idx': int(dst[i]),\n",
    "                    'src_ip': src_ip[i],\n",
    "                    'dst_ip': dst_ip[i],\n",
    "                    'y_true': int(y_true_np[i]),\n",
    "                    'y_pred': int(y_pred_np[i]),\n",
    "                    'p1': float(probs_np[i]),\n",
    "                    'margin': float(margin_np[i]),\n",
    "                    'loss_ce': float(loss_np[i]),\n",
    "                })\n",
    "    df_pred = pd.DataFrame(rows).sort_values(['bin','loss_ce'], ascending=[True, False])\n",
    "    return df_pred\n"
   ],
   "id": "8888a7f9276beff8",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:54:06.732978Z",
     "start_time": "2025-11-06T07:54:06.720158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bin_metrics(df_pred):\n",
    "    out = []\n",
    "    for b, g in df_pred.groupby('bin'):\n",
    "        y, yhat = g['y_true'].values, g['y_pred'].values\n",
    "        p1 = g['p1'].values\n",
    "        tp = ((y==1)&(yhat==1)).sum()\n",
    "        fp = ((y==0)&(yhat==1)).sum()\n",
    "        tn = ((y==0)&(yhat==0)).sum()\n",
    "        fn = ((y==1)&(yhat==0)).sum()\n",
    "        acc = (tp+tn)/max(1,len(g))\n",
    "        prec = tp/max(1,tp+fp)\n",
    "        rec  = tp/max(1,tp+fn)\n",
    "        f1   = 2*prec*rec/max(1e-9,prec+rec)\n",
    "        try:\n",
    "            auc = roc_auc_score(y, p1) if (y.min()!=y.max()) else np.nan\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        out.append({'bin': b, 'n': len(g), 'acc':acc, 'precision':prec,\n",
    "                    'recall':rec, 'f1':f1, 'auc':auc, 'fp':fp, 'fn':fn})\n",
    "    return pd.DataFrame(out).sort_values('bin')\n"
   ],
   "id": "83223fbac837ec6e",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:54:09.101067Z",
     "start_time": "2025-11-06T07:54:09.092758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def top_false_negatives(df_pred, k=20):\n",
    "    return df_pred[(df_pred.y_true==1)&(df_pred.y_pred==0)].sort_values('margin').head(k)\n",
    "\n",
    "def top_false_positives(df_pred, k=20):\n",
    "    return df_pred[(df_pred.y_true==0)&(df_pred.y_pred==1)].sort_values('p1', ascending=False).head(k)\n"
   ],
   "id": "412538aceee5d4d7",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:54:10.113731Z",
     "start_time": "2025-11-06T07:54:10.107873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def offender_table(df_pred, by='src_ip', k=30):\n",
    "    g = df_pred.groupby(by).apply(\n",
    "        lambda t: pd.Series({\n",
    "            'n': len(t),\n",
    "            'fn': int(((t.y_true==1)&(t.y_pred==0)).sum()),\n",
    "            'fp': int(((t.y_true==0)&(t.y_pred==1)).sum()),\n",
    "            'err_rate': float((t.y_true!=t.y_pred).mean())\n",
    "        })\n",
    "    ).reset_index()\n",
    "    g['fn_share'] = g['fn']/g['n'].clip(lower=1)\n",
    "    g['fp_share'] = g['fp']/g['n'].clip(lower=1)\n",
    "    return g.sort_values(['err_rate','n'], ascending=[False,False]).head(k)\n"
   ],
   "id": "f0d0167ac5a69a03",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:54:11.761059Z",
     "start_time": "2025-11-06T07:54:11.751656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def tune_threshold(df_pred, fn_cost=5.0, fp_cost=1.0):\n",
    "    \"\"\"\n",
    "    Choose threshold that minimizes cost = fn_cost*FN + fp_cost*FP.\n",
    "    \"\"\"\n",
    "    y = df_pred['y_true'].values\n",
    "    p = df_pred['p1'].values\n",
    "    # Sweep thresholds via precision-recall curve\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    # Convert to FPs/FNs given counts\n",
    "    P = (y==1).sum()\n",
    "    N = (y==0).sum()\n",
    "    # recall = TP/P -> TP = recall*P\n",
    "    # precision = TP/(TP+FP) -> FP = TP*(1/precision - 1)\n",
    "    TP = rec * P\n",
    "    FP = TP * (1/np.clip(prec,1e-9,None) - 1)\n",
    "    FN = P - TP\n",
    "    cost = fn_cost*FN + fp_cost*FP\n",
    "    idx = np.nanargmin(cost)\n",
    "    best_thr = thr[idx-1] if idx>0 and idx-1 < len(thr) else 0.5\n",
    "    stats = {'best_thr': float(best_thr), 'cost': float(cost[idx]),\n",
    "             'recall': float(rec[idx]), 'precision': float(prec[idx])}\n",
    "    return best_thr, stats\n"
   ],
   "id": "8bf0698450c1759c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:54:14.075254Z",
     "start_time": "2025-11-06T07:54:14.067852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def error_by_feature_quantiles(df_joined, feature, q=10):\n",
    "    \"\"\"\n",
    "    df_joined: df_pred merged with raw edge features for the same edge rows\n",
    "               must contain columns: feature, y_true, y_pred\n",
    "    \"\"\"\n",
    "    cuts = pd.qcut(df_joined[feature], q=q, duplicates='drop')\n",
    "    g = df_joined.groupby(cuts).apply(lambda t: pd.Series({\n",
    "        'n': len(t),\n",
    "        'err_rate': float((t.y_true!=t.y_pred).mean()),\n",
    "        'fn_rate': float(((t.y_true==1)&(t.y_pred==0)).mean()),\n",
    "        'fp_rate': float(((t.y_true==0)&(t.y_pred==1)).mean()),\n",
    "        'avg_p1': float(t['p1'].mean())\n",
    "    })).reset_index().rename(columns={feature:'bin_'+feature})\n",
    "    return g.sort_values('err_rate', ascending=False)\n"
   ],
   "id": "7236dd08b931049a",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T07:57:30.622414Z",
     "start_time": "2025-11-06T07:54:15.316959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After each test epoch:\n",
    "idx2ip_train = {v:k for k,v in train_ip2idx.items()}\n",
    "idx2ip_test  = {v:k for k,v in test_ip2idx.items()}\n",
    "\n",
    "df_train_pred = collect_edge_predictions(model, train_snaps, idx2ip_train, device=device)\n",
    "df_test_pred  = collect_edge_predictions(model, test_snaps,  idx2ip_test,  device=device)\n",
    "\n",
    "print(\"\\n=== Per-bin (TEST) ===\")\n",
    "print(bin_metrics(df_test_pred).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Top FN (TEST) ===\")\n",
    "print(top_false_negatives(df_test_pred, k=15)[['bin','src_ip','dst_ip','p1','margin','loss_ce']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Top FP (TEST) ===\")\n",
    "print(top_false_positives(df_test_pred, k=15)[['bin','src_ip','dst_ip','p1','margin','loss_ce']].to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Offenders by src_ip (TEST) ===\")\n",
    "print(offender_table(df_test_pred, by='src_ip', k=20).to_string(index=False))\n",
    "\n",
    "# Threshold tuning globally (you can also do this per bin)\n",
    "best_thr, stats = tune_threshold(df_train_pred, fn_cost=5.0, fp_cost=1.0)\n",
    "print(f\"\\n[Threshold] best {best_thr:.3f} | recall={stats['recall']:.3f} precision={stats['precision']:.3f}\")\n"
   ],
   "id": "ed075f25a1a45939",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-bin (TEST) ===\n",
      "    bin      n      acc  precision  recall       f1      auc  fp  fn\n",
      "5064053  32944 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064054  70503 0.999943   0.000000     0.0 0.000000      NaN   4   0\n",
      "5064055  64176 0.999875   0.000000     0.0 0.000000      NaN   8   0\n",
      "5064056  58299 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064057  51760 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064058  52064 0.999904   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064059  57539 0.999896   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064060  62764 0.999920   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064061  55622 0.999910   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064062  48847 0.999877   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064063  53442 0.999888   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064064  49436 0.999899   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064065  51642 0.999903   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064066  60733 0.999901   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064067  57174 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064068  52293 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064069  59848 0.999916   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064070  59637 0.999899   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064071  62753 0.999920   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064072  53775 0.999907   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064073  43875 0.999886   0.951923     1.0 0.975369 1.000000   5   0\n",
      "5064074  45892 0.999978   0.995984     1.0 0.997988 1.000000   1   0\n",
      "5064075  44773 1.000000   1.000000     1.0 1.000000 1.000000   0   0\n",
      "5064076  45537 1.000000   1.000000     1.0 1.000000 1.000000   0   0\n",
      "5064077  54076 1.000000   1.000000     1.0 1.000000 1.000000   0   0\n",
      "5064078  55743 1.000000   1.000000     1.0 1.000000 1.000000   0   0\n",
      "5064079  54413 1.000000   1.000000     1.0 1.000000 1.000000   0   0\n",
      "5064080  51601 0.999961   0.987013     1.0 0.993464 0.999998   2   0\n",
      "5064081  54777 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064082  50197 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064083  62723 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064084  54472 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064085  48650 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064086  49696 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064087  55657 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064088  50498 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064089  62777 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064090  52265 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064091  53155 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064092  53824 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064093  58323 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064094  53920 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064095  52932 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064096  45609 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064097  32988 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064098  36489 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064099  34485 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064100  35114 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064101  43441 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064102  37069 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064103  28788 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064104  31590 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064105  28939 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064106  32243 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064107  39275 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064108  32838 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064109  42749 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064110  39971 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064111  41548 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064112  48633 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064113  50714 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064114  47107 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064115  48761 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064116  48123 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064117  58579 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064118  55440 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064119  59809 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064120  49326 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064121  49480 0.999899   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064122 137468 0.999920   0.999879     1.0 0.999939 1.000000  11   0\n",
      "5064123 171193 0.999936   0.999908     1.0 0.999954 1.000000  11   0\n",
      "5064124 170777 0.999918   0.999883     1.0 0.999942 1.000000  14   0\n",
      "5064125 166969 0.999910   0.999872     1.0 0.999936 1.000000  15   0\n",
      "5064126 155460 0.999891   0.999840     1.0 0.999920 1.000000  17   0\n",
      "5064127 152077 0.999895   0.999849     1.0 0.999924 1.000000  16   0\n",
      "5064128 150493 0.999874   0.999808     1.0 0.999904 1.000000  19   0\n",
      "5064129 145946 0.999884   0.999819     1.0 0.999909 1.000000  17   0\n",
      "5064130 130749 0.999862   0.999775     1.0 0.999888 1.000000  18   0\n",
      "5064131 123040 0.999854   0.999751     1.0 0.999876 1.000000  18   0\n",
      "5064132 116253 0.999854   0.999754     1.0 0.999877 1.000000  17   0\n",
      "5064133  52589 0.999639   0.997605     1.0 0.998801 1.000000  19   0\n",
      "5064134  53697 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064135  50335 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064136  53560 0.999981   0.000000     0.0 0.000000      NaN   1   0\n",
      "5064137  49424 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064138  44371 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064139  42350 0.999858   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064140  48237 0.999896   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064141  46218 0.999870   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064142  48536 0.999897   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064143  50180 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064144  41556 0.999856   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064145  49568 0.999879   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064146  52372 0.999905   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064147  55699 0.999892   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064148  52050 0.999885   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064149  54668 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064150  50061 0.999880   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064151  54183 0.999908   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064152  56225 0.999893   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064153  55581 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064154  55288 0.999910   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064155  58087 0.999897   0.000000     0.0 0.000000      NaN   6   0\n",
      "5064156  40848 0.999829   0.000000     0.0 0.000000      NaN   7   0\n",
      "5064157  33469 0.999851   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064158  39827 0.999874   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064159  44894 0.999933   0.000000     0.0 0.000000      NaN   3   0\n",
      "5064160  33537 0.999851   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064161  42462 0.999882   0.000000     0.0 0.000000      NaN   5   0\n",
      "5064162   7583 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064163   2754 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064164    883 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064165    690 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064166    565 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064167    706 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064168    813 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064169   1485 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064170    627 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064171    856 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064172    319 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064173    577 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064174    662 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064175    567 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064176    158 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064177     88 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064178    106 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064179    125 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064180    130 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064181     79 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064182     89 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064183     68 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064184     69 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064185    108 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064186    105 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064187     77 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064188     79 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064189     71 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064190     72 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064191     52 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064192     72 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064193     68 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064194     45 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064195     52 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064196     63 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064197     59 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064198     42 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "5064199     17 1.000000   0.000000     0.0 0.000000      NaN   0   0\n",
      "\n",
      "=== Top FN (TEST) ===\n",
      "Empty DataFrame\n",
      "Columns: [bin, src_ip, dst_ip, p1, margin, loss_ce]\n",
      "Index: []\n",
      "\n",
      "=== Top FP (TEST) ===\n",
      "    bin          src_ip       dst_ip       p1    margin   loss_ce\n",
      "5064080 131.202.242.193 172.31.69.28 0.999969 10.373086 10.373117\n",
      "5064080 131.202.242.193 172.31.69.28 0.999807  8.551397  8.551591\n",
      "5064161    172.31.69.28 52.14.77.172 0.999059  6.967137  6.968079\n",
      "5064161    172.31.69.28  13.58.82.37 0.999055  6.963649  6.964594\n",
      "5064161    172.31.69.28  13.58.42.57 0.999055  6.963649  6.964594\n",
      "5064161    172.31.69.28  13.58.42.57 0.999055  6.963649  6.964594\n",
      "5064161    172.31.69.28  13.58.82.37 0.999055  6.963649  6.964594\n",
      "5064073    172.31.69.28 52.14.77.172 0.999020  6.927329  6.928308\n",
      "5064073    172.31.69.28  13.58.82.37 0.999017  6.923788  6.924772\n",
      "5064073    172.31.69.28  13.58.42.57 0.999017  6.923788  6.924772\n",
      "5064073    172.31.69.28  13.58.42.57 0.999017  6.923788  6.924772\n",
      "5064073    172.31.69.28  13.58.82.37 0.999017  6.923788  6.924772\n",
      "5064066    172.31.69.28 52.14.77.172 0.998849  6.765939  6.767091\n",
      "5064066    172.31.69.28 52.14.77.172 0.998849  6.765939  6.767091\n",
      "5064066    172.31.69.28  13.58.42.57 0.998845  6.762442  6.763598\n",
      "\n",
      "=== Offenders by src_ip (TEST) ===\n",
      "         src_ip        n  fn    fp  err_rate  fn_share  fp_share\n",
      "   172.31.69.28    561.0 0.0 269.0  0.479501       0.0  0.479501\n",
      "131.202.242.193     15.0 0.0   4.0  0.266667       0.0  0.266667\n",
      "  182.61.17.188    319.0 0.0  50.0  0.156740       0.0  0.156740\n",
      " 167.114.76.174    426.0 0.0  54.0  0.126761       0.0  0.126761\n",
      "  80.211.10.186    230.0 0.0  17.0  0.073913       0.0  0.073913\n",
      "  51.255.170.39     83.0 0.0   1.0  0.012048       0.0  0.012048\n",
      "   66.163.3.197   1431.0 0.0   2.0  0.001398       0.0  0.001398\n",
      " 18.216.200.189 109484.0 0.0   0.0  0.000000       0.0  0.000000\n",
      " 18.218.229.235 109311.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "  18.218.115.60 108828.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "   18.218.11.51 108509.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "   18.216.24.42 108238.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "     18.219.9.1 108233.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "   18.219.32.43 108219.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "  18.218.55.126 108089.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "  52.14.136.135 107666.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "    18.219.5.43 107446.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "     62.1.169.9  61424.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "   212.92.116.6  41228.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "   5.188.10.146  38603.0 0.0   0.0  0.000000       0.0  0.000000\n",
      "\n",
      "[Threshold] best 0.000 | recall=1.000 precision=1.000\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, we will see which IPs repeat in both training and test set.",
   "id": "9d93fe9523a26c6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
