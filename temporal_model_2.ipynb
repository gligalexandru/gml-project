{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50dfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aea76a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"custom_features_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7d5bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Timestamp', 'Src IP', 'Dst IP', 'Bwd Packet Length Min', 'Protocol_6',\n",
       "       'Bwd Packets/s', 'FWD Init Win Bytes', 'Packet Length Std',\n",
       "       'FIN Flag Count', 'SrcPortRange_registered', 'Packet Length Min',\n",
       "       'Fwd Seg Size Min', 'DstPortRange_well_known', 'Bwd IAT Total',\n",
       "       'SYN Flag Count', 'Bwd Packet Length Std', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_df = data[:int(len(data)*split_ratio)]\n",
    "test_df = data[int(len(data)*split_ratio):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc50049",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_COL = \"Timestamp\"\n",
    "SRC_COL = \"Src IP\"\n",
    "DST_COL = \"Dst IP\"\n",
    "LABEL_COL = \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e66bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_COLS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05f7805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_SECONDS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a05b451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62224b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_posenc(epoch_seconds: np.ndarray, periods=(60, 300, 3600, 86400)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    epoch_seconds: shape [E]\n",
    "    returns: shape [E, 2*len(periods)]\n",
    "    \"\"\"\n",
    "    s = np.asarray(epoch_seconds, dtype=float).reshape(-1, 1)   # [E, 1]\n",
    "    feats = []\n",
    "    for P in periods:\n",
    "        w = 2.0 * math.pi / float(P)\n",
    "        feats.append(np.sin(w * s))    # [E, 1]\n",
    "        feats.append(np.cos(w * s))    # [E, 1]\n",
    "    return np.concatenate(feats, axis=1)  # [E, 2*len(periods)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7929b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_snapshots(df: pd.DataFrame,\n",
    "                    edge_cols,\n",
    "                    fit_scaler: bool,\n",
    "                    scaler_edge: StandardScaler | None = None,\n",
    "                    bin_seconds: int = 300,\n",
    "                    device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      snapshots: list[Data] (PyG) in time order\n",
    "      ip2idx: dict mapping IP -> node index (stable across this split)\n",
    "      scaler_edge: fitted StandardScaler for edge features\n",
    "      edge_cols_kept: list of columns used (existing + non-NA) + time enc names\n",
    "    \"\"\"\n",
    "    ID_COLS = [SRC_COL, DST_COL, TIMESTAMP_COL]\n",
    "    cols_needed = [c for c in ID_COLS + edge_cols + [LABEL_COL] if c in df.columns]\n",
    "\n",
    "    df = df[cols_needed].dropna(subset=[SRC_COL, DST_COL])\n",
    "    df = bin_time(df, bin_seconds=bin_seconds)\n",
    "\n",
    "    # Edge feature scaler\n",
    "    if scaler_edge is None:\n",
    "        scaler_edge = StandardScaler()\n",
    "        fit_scaler = True\n",
    "    if fit_scaler:\n",
    "        scaler_edge.fit(df[edge_cols].astype(float).values)\n",
    "\n",
    "    # Stable node indexing across all snapshots\n",
    "    ips = pd.Index(pd.unique(pd.concat([df[SRC_COL], df[DST_COL]])))\n",
    "    ip2idx = {ip: i for i, ip in enumerate(ips)}\n",
    "\n",
    "    snapshots = []\n",
    "    prev_activity = defaultdict(int)  # 1-bin lag activity per node\n",
    "\n",
    "    for b, g in df.sort_values('_bin').groupby('_bin'):\n",
    "        # Nodes -> ids\n",
    "        src = g[SRC_COL].map(ip2idx).astype(int).values\n",
    "        dst = g[DST_COL].map(ip2idx).astype(int).values\n",
    "        edge_index = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
    "\n",
    "        # Edge features = scaled numeric + time encodings\n",
    "        eX = scaler_edge.transform(g[edge_cols].astype(float).values)\n",
    "        tfe = time_posenc(g['_epoch'].values)  # [E, 2*len(periods)]\n",
    "        edge_attr = torch.tensor(np.hstack([eX, tfe]), dtype=torch.float)\n",
    "\n",
    "        # Labels (edge-level)\n",
    "        y = torch.tensor(g[LABEL_COL].astype(int).values, dtype=torch.long)\n",
    "\n",
    "        # Node features (inductive): in/out/total degree + prev-bin activity\n",
    "        n_nodes = len(ip2idx)\n",
    "        out_deg = np.bincount(src, minlength=n_nodes)\n",
    "        in_deg = np.bincount(dst, minlength=n_nodes)\n",
    "        deg = (out_deg + in_deg).reshape(-1, 1)\n",
    "        node_feat = np.hstack([\n",
    "            in_deg.reshape(-1, 1),\n",
    "            out_deg.reshape(-1, 1),\n",
    "            deg,\n",
    "            np.array([prev_activity[i] for i in range(n_nodes)]).reshape(-1, 1)\n",
    "        ])\n",
    "        node_feat = np.log1p(node_feat)  # stabilize scale\n",
    "        x = torch.tensor(node_feat, dtype=torch.float)\n",
    "\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y\n",
    "        )\n",
    "        data._bin = int(b)\n",
    "        snapshots.append(data)\n",
    "\n",
    "        # Update prev activity\n",
    "        touched = np.bincount(np.concatenate([src, dst]), minlength=n_nodes)\n",
    "        for i, c in enumerate(touched):\n",
    "            prev_activity[i] = int(c)\n",
    "\n",
    "    time_names = [f\"time_{i}\" for i in range(tfe.shape[1])] if len(snapshots) > 0 else []\n",
    "    return snapshots, ip2idx, scaler_edge, edge_cols + time_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "388787e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightGRU_H(nn.Module):\n",
    "    \"\"\"EGCU-H: evolve weights using current node features + previous weights.\"\"\"\n",
    "    def __init__(self, in_feat_dim, w_in_dim, w_out_dim):\n",
    "        super().__init__()\n",
    "        self.w_in_dim = w_in_dim\n",
    "        self.w_out_dim = w_out_dim\n",
    "        self.h_dim = w_in_dim * w_out_dim\n",
    "        self.xproj = nn.Linear(in_feat_dim, self.h_dim)\n",
    "        self.gru = nn.GRUCell(self.h_dim, self.h_dim)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        x_sum = x_t.mean(dim=0, keepdim=True)           # [1, F]\n",
    "        x_in = self.xproj(x_sum).squeeze(0)             # [h_dim]\n",
    "        h_t = self.gru(x_in, h_prev)                    # [h_dim]\n",
    "        W_t = h_t.view(self.w_out_dim, self.w_in_dim)\n",
    "        return W_t, h_t\n",
    "\n",
    "\n",
    "class WeightGRU_O(nn.Module):\n",
    "    \"\"\"EGCU-O: evolve weights only from previous weights.\"\"\"\n",
    "    def __init__(self, w_in_dim, w_out_dim):\n",
    "        super().__init__()\n",
    "        self.w_in_dim = w_in_dim\n",
    "        self.w_out_dim = w_out_dim\n",
    "        self.h_dim = w_in_dim * w_out_dim\n",
    "        self.gru = nn.GRUCell(self.h_dim, self.h_dim)\n",
    "\n",
    "    def forward(self, _x_t_unused, h_prev):\n",
    "        zeros = torch.zeros_like(h_prev)\n",
    "        h_t = self.gru(zeros, h_prev)\n",
    "        W_t = h_t.view(self.w_out_dim, self.w_in_dim)\n",
    "        return W_t, h_t\n",
    "\n",
    "\n",
    "class EvolveGCNEdgeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in for GraphTimeEdgeClassifier:\n",
    "    - forward(data) -> logits [E, C]\n",
    "    - call reset_state() at the start of each snapshot sequence\n",
    "    \"\"\"\n",
    "    def __init__(self, in_node, in_edge, hidden=64, out_hidden=64,\n",
    "                 num_layers=2, dropout=0.1, variant='H', num_classes=2):\n",
    "        super().__init__()\n",
    "        assert num_layers >= 1\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # GCN layers (weights updated each step)\n",
    "        self.convs = nn.ModuleList()\n",
    "        dims = [in_node] + [hidden]*(num_layers-1) + [out_hidden]\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(GCNConv(dims[i], dims[i+1], bias=False))\n",
    "\n",
    "        # Evolvers\n",
    "        self.variant = variant.upper()\n",
    "        self.evolvers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_dim = dims[i]\n",
    "            out_dim = dims[i+1]\n",
    "            if self.variant == 'H':\n",
    "                self.evolvers.append(WeightGRU_H(in_dim, w_in_dim=in_dim, w_out_dim=out_dim))\n",
    "            elif self.variant == 'O':\n",
    "                self.evolvers.append(WeightGRU_O(w_in_dim=in_dim, w_out_dim=out_dim))\n",
    "            else:\n",
    "                raise ValueError(\"variant must be 'H' or 'O'\")\n",
    "\n",
    "        # Hidden states (flattened weights)\n",
    "        self._states = [None]*num_layers\n",
    "        self._on_device = None  # track device for init\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2*out_hidden + in_edge, out_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_hidden, num_classes)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reset_state(self, hard=True):\n",
    "        \"\"\"Reinitialize the evolving weights (call at start of a sequence).\"\"\"\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            device = conv.lin.weight.device\n",
    "            W = torch.empty(conv.out_channels, conv.in_channels, device=device)\n",
    "            nn.init.xavier_uniform_(W)\n",
    "            self._states[i] = W.flatten().detach().clone()\n",
    "            conv.lin.weight.copy_(W)\n",
    "\n",
    "    def detach_state(self):\n",
    "        \"\"\"Detach hidden states between steps (for truncated BPTT if needed).\"\"\"\n",
    "        self._states = [h.detach() for h in self._states]\n",
    "\n",
    "    def _evolve_and_set(self, layer_idx, x_t):\n",
    "        h_prev = self._states[layer_idx]\n",
    "        evolver = self.evolvers[layer_idx]\n",
    "        W_t, h_t = evolver(x_t, h_prev)\n",
    "        self._states[layer_idx] = h_t\n",
    "        with torch.no_grad():\n",
    "            self.convs[layer_idx].lin.weight.copy_(W_t)\n",
    "        return W_t\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for l, conv in enumerate(self.convs):\n",
    "            self._evolve_and_set(l, x)\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.act(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        src, dst = edge_index\n",
    "        z = torch.cat([x[src], x[dst], edge_attr], dim=1)\n",
    "        logits = self.edge_mlp(z)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23332da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_edge_cols(df: pd.DataFrame):\n",
    "    if EDGE_COLS is not None:\n",
    "        return [c for c in EDGE_COLS if c in df.columns]\n",
    "    # auto-detect numeric columns (exclude ids and label)\n",
    "    excluded = {TIMESTAMP_COL, SRC_COL, DST_COL, LABEL_COL}\n",
    "    cand = []\n",
    "    for c in df.columns:\n",
    "        if c in excluded: \n",
    "            continue\n",
    "        # keep only numeric-like columns (ignore strings/categoricals)\n",
    "        try:\n",
    "            pd.to_numeric(df[c].dropna().iloc[:10])\n",
    "            cand.append(c)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92438efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 edge feature columns: ['Bwd Packet Length Min', 'Protocol_6', 'Bwd Packets/s', 'FWD Init Win Bytes', 'Packet Length Std', 'FIN Flag Count', 'SrcPortRange_registered', 'Packet Length Min', 'Fwd Seg Size Min', 'DstPortRange_well_known'] ...\n"
     ]
    }
   ],
   "source": [
    "edge_cols = pick_edge_cols(train_df)\n",
    "print(f\"Using {len(edge_cols)} edge feature columns:\", edge_cols[:10], \"...\" if len(edge_cols) > 10 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f984fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building snapshots...\n",
      "Train snapshots: 673 | Test snapshots: 147\n",
      "Node feature dim: 4 | Edge feature dim: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Building snapshots...\")\n",
    "scaler = StandardScaler().fit(train_df[edge_cols].astype(float).values)\n",
    "\n",
    "train_snaps, train_ip2idx, scaler, edge_cols_full = build_snapshots(\n",
    "    train_df, edge_cols=edge_cols, fit_scaler=False, scaler_edge=scaler,\n",
    "    bin_seconds=BIN_SECONDS, device=DEVICE\n",
    ")\n",
    "test_snaps,  test_ip2idx,  _,      _ = build_snapshots(\n",
    "    test_df,  edge_cols=edge_cols, fit_scaler=False, scaler_edge=scaler,\n",
    "    bin_seconds=BIN_SECONDS, device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"Train snapshots: {len(train_snaps)} | Test snapshots: {len(test_snaps)}\")\n",
    "in_node_dim = train_snaps[0].x.size(1)\n",
    "in_edge_dim = train_snaps[0].edge_attr.size(1)\n",
    "print(\"Node feature dim:\", in_node_dim, \"| Edge feature dim:\", in_edge_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8655b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_sequence(model, optimizer, snapshots, device=\"cpu\", detach_every=None):\n",
    "    \"\"\"\n",
    "    Train across an ordered list of snapshots (one sequence).\n",
    "    detach_every: if set (int), detach model state every N steps (truncated BPTT)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.reset_state(hard=False)  # keep evolving but ensure conv weights are set\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_edges = 0\n",
    "\n",
    "    for t, data in enumerate(snapshots):\n",
    "        data = data.to(device)\n",
    "        logits = model(data)\n",
    "        loss = F.cross_entropy(logits, data.y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * data.y.numel()\n",
    "        total_edges += int(data.y.numel())\n",
    "\n",
    "        if detach_every and (t + 1) % detach_every == 0:\n",
    "            model.detach_state()\n",
    "\n",
    "    return total_loss / max(total_edges, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6e52bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_sequences(model, sequences, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "\n",
    "    for snaps in sequences:\n",
    "        model.reset_state(hard=False)\n",
    "        for data in snaps:\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            y = data.y.cpu().numpy()\n",
    "            all_preds.append(pred)\n",
    "            all_true.append(y)\n",
    "    y_true = np.concatenate(all_true) if all_true else np.array([])\n",
    "    y_pred = np.concatenate(all_preds) if all_preds else np.array([])\n",
    "    acc = accuracy_score(y_true, y_pred) if y_true.size else 0.0\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1]) if y_true.size else np.zeros((2, 2), dtype=int)\n",
    "    return acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6615965",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvolveGCNEdgeClassifier(\n",
    "    in_node=in_node_dim,\n",
    "    in_edge=in_edge_dim,\n",
    "    hidden=64,\n",
    "    out_hidden=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    variant='H',      # 'H' (EGCU-H) or 'O' (EGCU-O)\n",
    "    num_classes=2\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Treat train/test snapshots each as a single ordered sequence.\n",
    "# If you have multiple sequences, wrap them as list-of-lists.\n",
    "train_sequences = [train_snaps]\n",
    "test_sequences  = [test_snaps]\n",
    "\n",
    "EPOCHS = 10\n",
    "DETACH_EVERY = 5  # optional truncated BPTT across long sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b115f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gliga\\AppData\\Local\\Temp\\ipykernel_83244\\893500266.py:21: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  total_loss += float(loss) * data.y.numel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] loss=15.5214 | test acc=0.8326\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5391219       0]\n",
      " [1084194       0]]\n",
      "[Epoch 02] loss=6.9870 | test acc=0.8326\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5391219       0]\n",
      " [1084194       0]]\n",
      "[Epoch 03] loss=1.0274 | test acc=0.8326\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5391219       0]\n",
      " [1084194       0]]\n",
      "[Epoch 04] loss=0.8291 | test acc=0.9994\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5389077    2142]\n",
      " [   1901 1082293]]\n",
      "[Epoch 05] loss=0.3244 | test acc=0.9991\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5387353    3866]\n",
      " [   1901 1082293]]\n",
      "[Epoch 06] loss=0.0382 | test acc=0.8326\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5391219       0]\n",
      " [1084194       0]]\n",
      "[Epoch 07] loss=0.2955 | test acc=0.9986\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5383761    7458]\n",
      " [   1901 1082293]]\n",
      "[Epoch 08] loss=14.1701 | test acc=0.9928\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5346317   44902]\n",
      " [   1901 1082293]]\n",
      "[Epoch 09] loss=0.0216 | test acc=0.9986\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5383820    7399]\n",
      " [   1901 1082293]]\n",
      "[Epoch 10] loss=0.0698 | test acc=0.9916\n",
      "Confusion matrix (rows=true [0,1], cols=pred [0,1]):\n",
      " [[5338980   52239]\n",
      " [   1901 1082293]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # One pass over the (single) train sequence\n",
    "    train_loss = train_one_sequence(model, optimizer, train_snaps, device=DEVICE, detach_every=DETACH_EVERY)\n",
    "\n",
    "    # Eval on test\n",
    "    acc, cm = eval_sequences(model, test_sequences, device=DEVICE)\n",
    "\n",
    "    print(f\"[Epoch {epoch:02d}] loss={train_loss:.4f} | test acc={acc:.4f}\")\n",
    "    print(\"Confusion matrix (rows=true [0,1], cols=pred [0,1]):\\n\", cm)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60a79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
